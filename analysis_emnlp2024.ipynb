{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12447, 16)\n",
      "Index(['hatespeech', 'counterspeech', 'predicted_counterspeech', 'csType',\n",
      "       'source', 'uuid', 'aggressiveness_score', 'coherence_score',\n",
      "       'relevance_score', 'suitableness_score', 'gpt-4_relevance_score',\n",
      "       'gpt-4_coherence_score', 'gpt-4_aggressiveness_score',\n",
      "       'gpt-4_suitableness_score', 'used_for_interannotations',\n",
      "       'overall_score'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>counterspeech</th>\n",
       "      <th>predicted_counterspeech</th>\n",
       "      <th>csType</th>\n",
       "      <th>source</th>\n",
       "      <th>uuid</th>\n",
       "      <th>aggressiveness_score</th>\n",
       "      <th>coherence_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>suitableness_score</th>\n",
       "      <th>gpt-4_relevance_score</th>\n",
       "      <th>gpt-4_coherence_score</th>\n",
       "      <th>gpt-4_aggressiveness_score</th>\n",
       "      <th>gpt-4_suitableness_score</th>\n",
       "      <th>used_for_interannotations</th>\n",
       "      <th>overall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minorities vote to strip wealth and power from...</td>\n",
       "      <td>This is incorrect. jews could never be able to...</td>\n",
       "      <td>People's right to vote is not determined by ra...</td>\n",
       "      <td>Informative</td>\n",
       "      <td>dialoGPT</td>\n",
       "      <td>16229ec9-79d2-4b81-86aa-7a81335584b2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          hatespeech  \\\n",
       "0  Minorities vote to strip wealth and power from...   \n",
       "\n",
       "                                       counterspeech  \\\n",
       "0  This is incorrect. jews could never be able to...   \n",
       "\n",
       "                             predicted_counterspeech       csType    source  \\\n",
       "0  People's right to vote is not determined by ra...  Informative  dialoGPT   \n",
       "\n",
       "                                   uuid  aggressiveness_score  \\\n",
       "0  16229ec9-79d2-4b81-86aa-7a81335584b2                     1   \n",
       "\n",
       "   coherence_score  relevance_score  suitableness_score  \\\n",
       "0                4                5                   2   \n",
       "\n",
       "   gpt-4_relevance_score  gpt-4_coherence_score  gpt-4_aggressiveness_score  \\\n",
       "0                      5                      3                           1   \n",
       "\n",
       "   gpt-4_suitableness_score  used_for_interannotations  overall_score  \n",
       "0                         3                       True            NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('/home/ameyh/cs-eval/annotations/annotations_final_pass2.csv')\n",
    "usecols1 = [x for x in df1.columns if 'prediction' not in x]\n",
    "df1 = df1[usecols1]\n",
    "print(df1.shape)\n",
    "print(df1.columns)\n",
    "df1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "used_for_interannotations\n",
       "False    10385\n",
       "True      2062\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['used_for_interannotations'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'obscenity_(pred_cs)', 'identity_attack_(pred_cs)',\n",
      "       'insult_(pred_cs)', 'bleu_1_(pred_cs, cs)', 'bleu_2_(pred_cs, cs)',\n",
      "       'cosine_similarity', 'rouge_l_(pred_cs, cs)', 'rouge_1_(pred_cs, cs)',\n",
      "       'rouge_2_(pred_cs, cs)', 'meteor_score_(pred_cs, cs)',\n",
      "       'bert_score_(hs, pred_cs)', 'toxicity_(pred_cs)',\n",
      "       'pc_score_(hs, pred_cs)', 'cd_score_(hs, pred_cs)',\n",
      "       'aq_score_(pred_cs)', 'bm25_score_(hs, pred_cs)',\n",
      "       'bert_score_(pred_cs, cs)', 'bm25_score_(pred_cs, cs)',\n",
      "       'bart_score_(pred_cs, cs)', 'bart_score_(cs,pred_cs)',\n",
      "       'papi_toxicity_score_(pred_cs)', 'papi_threat_score_(pred_cs)',\n",
      "       'papi_insult_score_(pred_cs)', 'papi_profanity_score_(pred_cs)',\n",
      "       'papi_identity_attack_score_(pred_cs)', 'gpt_relevance_score',\n",
      "       'gpt_aggressiveness_score', 'gpt_coherence_score',\n",
      "       'gpt_suitableness_score', 'uuid'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>obscenity_(pred_cs)</th>\n",
       "      <th>identity_attack_(pred_cs)</th>\n",
       "      <th>insult_(pred_cs)</th>\n",
       "      <th>bleu_1_(pred_cs, cs)</th>\n",
       "      <th>bleu_2_(pred_cs, cs)</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>rouge_l_(pred_cs, cs)</th>\n",
       "      <th>rouge_1_(pred_cs, cs)</th>\n",
       "      <th>rouge_2_(pred_cs, cs)</th>\n",
       "      <th>...</th>\n",
       "      <th>papi_toxicity_score_(pred_cs)</th>\n",
       "      <th>papi_threat_score_(pred_cs)</th>\n",
       "      <th>papi_insult_score_(pred_cs)</th>\n",
       "      <th>papi_profanity_score_(pred_cs)</th>\n",
       "      <th>papi_identity_attack_score_(pred_cs)</th>\n",
       "      <th>gpt_relevance_score</th>\n",
       "      <th>gpt_aggressiveness_score</th>\n",
       "      <th>gpt_coherence_score</th>\n",
       "      <th>gpt_suitableness_score</th>\n",
       "      <th>uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.00014</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.033354</td>\n",
       "      <td>0.227242</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027207</td>\n",
       "      <td>0.006848</td>\n",
       "      <td>0.008501</td>\n",
       "      <td>0.013029</td>\n",
       "      <td>0.015255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16229ec9-79d2-4b81-86aa-7a81335584b2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  obscenity_(pred_cs)  identity_attack_(pred_cs)  insult_(pred_cs)  \\\n",
       "0   3             0.000019                    0.00014          0.000138   \n",
       "\n",
       "   bleu_1_(pred_cs, cs)  bleu_2_(pred_cs, cs)  cosine_similarity  \\\n",
       "0              0.225806              0.033354           0.227242   \n",
       "\n",
       "   rouge_l_(pred_cs, cs)  rouge_1_(pred_cs, cs)  rouge_2_(pred_cs, cs)  ...  \\\n",
       "0               0.177778               0.177778                    0.0  ...   \n",
       "\n",
       "   papi_toxicity_score_(pred_cs)  papi_threat_score_(pred_cs)  \\\n",
       "0                       0.027207                     0.006848   \n",
       "\n",
       "   papi_insult_score_(pred_cs)  papi_profanity_score_(pred_cs)  \\\n",
       "0                     0.008501                        0.013029   \n",
       "\n",
       "   papi_identity_attack_score_(pred_cs)  gpt_relevance_score  \\\n",
       "0                              0.015255                  1.0   \n",
       "\n",
       "   gpt_aggressiveness_score  gpt_coherence_score  gpt_suitableness_score  \\\n",
       "0                       2.0                  3.0                     2.0   \n",
       "\n",
       "                                   uuid  \n",
       "0  16229ec9-79d2-4b81-86aa-7a81335584b2  \n",
       "\n",
       "[1 rows x 31 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('/home/ameyh/cs-eval/annotations/annotations_final_metrics_calculated.csv')\n",
    "usecols2 = [x for x in df2.columns if x not in usecols1]\n",
    "usecols2.append('uuid')\n",
    "df2 = df2[usecols2]\n",
    "print(df2.columns)\n",
    "df2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11312, 46)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>counterspeech</th>\n",
       "      <th>predicted_counterspeech</th>\n",
       "      <th>csType</th>\n",
       "      <th>source</th>\n",
       "      <th>uuid</th>\n",
       "      <th>aggressiveness_score</th>\n",
       "      <th>coherence_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>suitableness_score</th>\n",
       "      <th>...</th>\n",
       "      <th>bart_score_(cs,pred_cs)</th>\n",
       "      <th>papi_toxicity_score_(pred_cs)</th>\n",
       "      <th>papi_threat_score_(pred_cs)</th>\n",
       "      <th>papi_insult_score_(pred_cs)</th>\n",
       "      <th>papi_profanity_score_(pred_cs)</th>\n",
       "      <th>papi_identity_attack_score_(pred_cs)</th>\n",
       "      <th>gpt_relevance_score</th>\n",
       "      <th>gpt_aggressiveness_score</th>\n",
       "      <th>gpt_coherence_score</th>\n",
       "      <th>gpt_suitableness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minorities vote to strip wealth and power from...</td>\n",
       "      <td>This is incorrect. jews could never be able to...</td>\n",
       "      <td>People's right to vote is not determined by ra...</td>\n",
       "      <td>Informative</td>\n",
       "      <td>dialoGPT</td>\n",
       "      <td>16229ec9-79d2-4b81-86aa-7a81335584b2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.327860</td>\n",
       "      <td>0.027207</td>\n",
       "      <td>0.006848</td>\n",
       "      <td>0.008501</td>\n",
       "      <td>0.013029</td>\n",
       "      <td>0.015255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The feminists have won. With so many women bei...</td>\n",
       "      <td>Replaced by what exactly? i think our populati...</td>\n",
       "      <td>Why do you believe women's rights are solely a...</td>\n",
       "      <td>Questioning</td>\n",
       "      <td>dialoGPT</td>\n",
       "      <td>1338eeaa-4332-477a-985c-a20d431ee171</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.295844</td>\n",
       "      <td>0.100457</td>\n",
       "      <td>0.006440</td>\n",
       "      <td>0.021675</td>\n",
       "      <td>0.012756</td>\n",
       "      <td>0.095764</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The feminists have won. With so many women bei...</td>\n",
       "      <td>This is absolutely the wrong stance to have on...</td>\n",
       "      <td>Feminism advocates for gender equality and rep...</td>\n",
       "      <td>Denouncing</td>\n",
       "      <td>dialoGPT</td>\n",
       "      <td>2b951b1b-88d5-4464-9d25-af013c071508</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.295999</td>\n",
       "      <td>0.049584</td>\n",
       "      <td>0.007107</td>\n",
       "      <td>0.012605</td>\n",
       "      <td>0.017641</td>\n",
       "      <td>0.048996</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          hatespeech  \\\n",
       "0  Minorities vote to strip wealth and power from...   \n",
       "1  The feminists have won. With so many women bei...   \n",
       "2  The feminists have won. With so many women bei...   \n",
       "\n",
       "                                       counterspeech  \\\n",
       "0  This is incorrect. jews could never be able to...   \n",
       "1  Replaced by what exactly? i think our populati...   \n",
       "2  This is absolutely the wrong stance to have on...   \n",
       "\n",
       "                             predicted_counterspeech       csType    source  \\\n",
       "0  People's right to vote is not determined by ra...  Informative  dialoGPT   \n",
       "1  Why do you believe women's rights are solely a...  Questioning  dialoGPT   \n",
       "2  Feminism advocates for gender equality and rep...   Denouncing  dialoGPT   \n",
       "\n",
       "                                   uuid  aggressiveness_score  \\\n",
       "0  16229ec9-79d2-4b81-86aa-7a81335584b2                     1   \n",
       "1  1338eeaa-4332-477a-985c-a20d431ee171                     1   \n",
       "2  2b951b1b-88d5-4464-9d25-af013c071508                     1   \n",
       "\n",
       "   coherence_score  relevance_score  suitableness_score  ...  \\\n",
       "0                4                5                   2  ...   \n",
       "1                3                4                   2  ...   \n",
       "2                4                5                   2  ...   \n",
       "\n",
       "   bart_score_(cs,pred_cs)  papi_toxicity_score_(pred_cs)  \\\n",
       "0                -3.327860                       0.027207   \n",
       "1                -3.295844                       0.100457   \n",
       "2                -3.295999                       0.049584   \n",
       "\n",
       "   papi_threat_score_(pred_cs)  papi_insult_score_(pred_cs)  \\\n",
       "0                     0.006848                     0.008501   \n",
       "1                     0.006440                     0.021675   \n",
       "2                     0.007107                     0.012605   \n",
       "\n",
       "   papi_profanity_score_(pred_cs)  papi_identity_attack_score_(pred_cs)  \\\n",
       "0                        0.013029                              0.015255   \n",
       "1                        0.012756                              0.095764   \n",
       "2                        0.017641                              0.048996   \n",
       "\n",
       "   gpt_relevance_score  gpt_aggressiveness_score  gpt_coherence_score  \\\n",
       "0                  1.0                       2.0                  3.0   \n",
       "1                  5.0                       1.0                  3.0   \n",
       "2                  5.0                       1.0                  4.0   \n",
       "\n",
       "   gpt_suitableness_score  \n",
       "0                     2.0  \n",
       "1                     2.0  \n",
       "2                     3.0  \n",
       "\n",
       "[3 rows x 46 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged = pd.merge(df1,df2[usecols2],on='uuid')\n",
    "print(df_merged.shape)\n",
    "df_merged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "used_for_interannotations\n",
       "False    9250\n",
       "True     2062\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged['used_for_interannotations'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['used_for_interannotations'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv('/home/ameyh/cs-eval/data/analysis_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for Illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hatespeech: Its always quite before the headscarves show up. Our neighbourhoods can no longer be the same.\n",
      "counterspeech_A: Isn't this view a reflection of ingrained stereotypes related to Islamophobia? By wearing the hijab, muslim women express their cultural and religious identity. It would be wrong to assume that Muslims are responsible for creating disturbance in communities.\n",
      "counterspeech_B: The entire notion of headscarves disrupting peace is unfounded. Why would our neighbourhoods no longer be same? Wearing headscarves is a matter of personal expression, much like much like any other choice in attire before going out.\n"
     ]
    }
   ],
   "source": [
    "hatespeech = \"Its always quite before the headscarves show up. Our neighbourhoods can no longer be the same.\"\n",
    "counterspeech_gold = \"Why would the presence of women wearing headscarves be seen as a disruption to quiet?\"\n",
    "counterspeech_A = \"Isn't this view a reflection of ingrained stereotypes related to Islamophobia? By wearing the hijab, muslim women express their cultural and religious identity. It would be wrong to assume that Muslims are responsible for creating disturbance in communities.\"\n",
    "counterspeech_B1 = \"Wearing headscarves is just a matter of personal and cultural expression, much like any other choice in attire before going out. It's about personal style and comfort, not about creating any disturbance or impacting the quiet. Let's respect everyone's choices in how they present themselves, including the decision to wear headscarves, and appreciate the variety it brings to our collective experiences.\"\n",
    "counterspeech_B = \"The entire notion of headscarves disrupting peace is unfounded. Why would our neighbourhoods no longer be same? Wearing headscarves is a matter of personal expression, much like much like any other choice in attire before going out.\"\n",
    "\n",
    "print(f\"Hatespeech: {hatespeech}\")\n",
    "print(f\"counterspeech_A: {counterspeech_A}\")\n",
    "print(f\"counterspeech_B: {counterspeech_B}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge Scores 1: [0.07999999580000022], [0.07999999580000022], [0.0]\n",
      "Rouge Scores 2: [0.21276595310095073], [0.25531914459031246], [0.07999999580000022]\n",
      "Size of input df: 1\n",
      "Cleaning test data\n",
      "--------------------------------------------------\n",
      "Calculating rouge score\n",
      "--------------------------------------------------\n",
      "Calculating Bleu1 and Bleu2 score\n",
      "--------------------------------------------------\n",
      "Calculating Meteor score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ameyh/.conda/envs/cs-eval/lib/python3.9/site-packages/datasets/load.py:752: FutureWarning: The repository for meteor contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/meteor/meteor.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package wordnet to /home/ameyh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ameyh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ameyh/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Calculating Cosine Similarity\n",
      "--------------------------------------------------\n",
      "Calculating Bert score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Calculating Bert score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Calculating Toxicity score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Toxicity inference on 1 data points: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 82.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Calculating PC Score\n",
      "--------------------------------------------------\n",
      "Calculating CD Score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.14it/s] | 0/1 [00:00<?, ?it/s]\n",
      "ClaimDetectionClient: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.14s/it]\n",
      "ArgumentQualityClient: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.20it/s]\n",
      "Running PC/CD/AQ inference on 1 data points: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Calculating BM25 Score\n",
      "--------------------------------------------------\n",
      "Calculating BM25 Score\n",
      "Size of input df: 1\n",
      "Cleaning test data\n",
      "--------------------------------------------------\n",
      "Calculating rouge score\n",
      "--------------------------------------------------\n",
      "Calculating Bleu1 and Bleu2 score\n",
      "--------------------------------------------------\n",
      "Calculating Meteor score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ameyh/.conda/envs/cs-eval/lib/python3.9/site-packages/datasets/load.py:752: FutureWarning: The repository for meteor contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/meteor/meteor.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package wordnet to /home/ameyh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ameyh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ameyh/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Calculating Cosine Similarity\n",
      "--------------------------------------------------\n",
      "Calculating Bert score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Calculating Bert score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Calculating Toxicity score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Toxicity inference on 1 data points: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 74.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Calculating PC Score\n",
      "--------------------------------------------------\n",
      "Calculating CD Score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.27it/s] | 0/1 [00:00<?, ?it/s]\n",
      "ClaimDetectionClient: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.18s/it]\n",
      "ArgumentQualityClient: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.17it/s]\n",
      "Running PC/CD/AQ inference on 1 data points: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Calculating BM25 Score\n",
      "--------------------------------------------------\n",
      "Calculating BM25 Score\n"
     ]
    }
   ],
   "source": [
    "from metrics import Metrics\n",
    "\n",
    "compute_metrics = Metrics()\n",
    "\n",
    "rouge_l, rouge_1, rouge_2 = compute_metrics.compute_rouge_score(predictions=[counterspeech_A], references=[hatespeech])\n",
    "print(f\"Rouge Scores 1: {rouge_l}, {rouge_1}, {rouge_2}\")\n",
    "\n",
    "rouge_l, rouge_1, rouge_2 = compute_metrics.compute_rouge_score(predictions=[counterspeech_B], references=[hatespeech])\n",
    "print(f\"Rouge Scores 2: {rouge_l}, {rouge_1}, {rouge_2}\")\n",
    "\n",
    "df1 = pd.DataFrame(columns=['hatespeech', 'counterspeech', 'predicted_counterspeech','csType'])\n",
    "df1['hatespeech'] = [hatespeech]\n",
    "df1['counterspeech'] = [hatespeech]\n",
    "df1['predicted_counterspeech'] = [counterspeech_A]\n",
    "df1['csType'] = ['Informative']\n",
    "\n",
    "df2 = pd.DataFrame(columns=['hatespeech', 'counterspeech', 'predicted_counterspeech'])\n",
    "df2['hatespeech'] = [hatespeech]\n",
    "df2['counterspeech'] = [hatespeech]\n",
    "df2['predicted_counterspeech'] = [counterspeech_B]\n",
    "df2['csType'] = ['Informative']\n",
    "\n",
    "\n",
    "df1 = compute_metrics.get_generation_metrics(df1)\n",
    "df2 = compute_metrics.get_generation_metrics(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "obscenity_(pred_cs)\n",
      "A: 0.001164\n",
      "B: 4.3e-05\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "identity_attack_(pred_cs)\n",
      "A: 0.126093\n",
      "B: 0.00013\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "insult_(pred_cs)\n",
      "A: 0.006968\n",
      "B: 0.00023\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "bleu_1_(pred_cs, cs)\n",
      "A: 0.092369\n",
      "B: 0.099138\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "bleu_2_(pred_cs, cs)\n",
      "A: 0.014335\n",
      "B: 0.015289\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "cosine_similarity\n",
      "A: 0.064217\n",
      "B: 0.253148\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "rouge_l_(pred_cs, cs)\n",
      "A: 0.08\n",
      "B: 0.297872\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "rouge_1_(pred_cs, cs)\n",
      "A: 0.08\n",
      "B: 0.340426\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "rouge_2_(pred_cs, cs)\n",
      "A: 0.0\n",
      "B: 0.12\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "meteor_score_(pred_cs, cs)\n",
      "A: 0.148515\n",
      "B: 0.466441\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "bert_score_(hs, pred_cs)\n",
      "A: 0.852666974067688\n",
      "B: 0.8761330246925354\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "toxicity_(pred_cs)\n",
      "A: 0.081709\n",
      "B: 0.000973\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "pc_score_(hs, pred_cs)\n",
      "A: -0.411776\n",
      "B: 0.889756\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "cd_score_(hs, pred_cs)\n",
      "A: 0.021082\n",
      "B: 0.515683\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "aq_score_(pred_cs)\n",
      "A: 0.364176\n",
      "B: 0.709486\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "bm25_score_(hs, pred_cs)\n",
      "A: -0.144059\n",
      "B: -1.551964\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "bert_score_(pred_cs, cs)\n",
      "A: 0.852666974067688\n",
      "B: 0.8761330246925354\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "bm25_score_(pred_cs, cs)\n",
      "A: -0.144059\n",
      "B: -1.551964\n"
     ]
    }
   ],
   "source": [
    "for col in df1.columns:\n",
    "    if col not in ['hatespeech', 'counterspeech', 'predicted_counterspeech', 'csType']:\n",
    "        print('- -'*20)\n",
    "        value_A = round(df1.iloc[0][col],6)\n",
    "        value_B = round(df2.iloc[0][col],6)\n",
    "        print(col)\n",
    "        print(f\"A: {value_A}\\nB: {value_B}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('/home/ameyh/counterspeech-EVAL/data/illustration_counterspeechA.csv',index=False)\n",
    "df2.to_csv('/home/ameyh/counterspeech-EVAL/data/illustration_counterspeechB.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/home/ameyh/counterspeech-EVAL/data/illustration_counterspeechA.csv')\n",
    "df2 = pd.read_csv('/home/ameyh/counterspeech-EVAL/data/illustration_counterspeechB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "obscenity_(pred_cs)\n",
      "A: 0.0011641413439065\n",
      "B: 4.31256448791828e-05\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "identity_attack_(pred_cs)\n",
      "A: 0.1260934174060821\n",
      "B: 0.0001299760479014\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "insult_(pred_cs)\n",
      "A: 0.006968037225306\n",
      "B: 0.0002301395434187\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "bleu_1_(pred_cs, cs)\n",
      "A: 0.0923694779116465\n",
      "B: 0.0991379310344827\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "bleu_2_(pred_cs, cs)\n",
      "A: 0.0143353308670557\n",
      "B: 0.0152891081667543\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "cosine_similarity\n",
      "A: 0.0642172600209856\n",
      "B: 0.2531476537622941\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "rouge_l_(pred_cs, cs)\n",
      "A: 0.0799999958000002\n",
      "B: 0.2978723360796741\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "rouge_1_(pred_cs, cs)\n",
      "A: 0.0799999958000002\n",
      "B: 0.3404255275690357\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "rouge_2_(pred_cs, cs)\n",
      "A: 0.0\n",
      "B: 0.1199999958000001\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "meteor_score_(pred_cs, cs)\n",
      "A: 0.1485148514851485\n",
      "B: 0.4664408866995073\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "bert_score_(hs, pred_cs)\n",
      "A: 0.852667\n",
      "B: 0.876133\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "toxicity_(pred_cs)\n",
      "A: 0.0817089974880218\n",
      "B: 0.0009729145676828\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "pc_score_(hs, pred_cs)\n",
      "A: -0.4117755591869354\n",
      "B: 0.8897556625306606\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "cd_score_(hs, pred_cs)\n",
      "A: 0.0210819281637668\n",
      "B: 0.515682578086853\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "aq_score_(pred_cs)\n",
      "A: 0.3641755878925323\n",
      "B: 0.7094855308532715\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "bm25_score_(hs, pred_cs)\n",
      "A: -0.1440590507588499\n",
      "B: -1.551964373605753\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "bert_score_(pred_cs, cs)\n",
      "A: 0.852667\n",
      "B: 0.876133\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "bm25_score_(pred_cs, cs)\n",
      "A: -0.1440590507588499\n",
      "B: -1.551964373605753\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "coherence_UniEval\n",
      "A: 0.8940779721837084\n",
      "B: 0.8899802952006514\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "relevance_UniEval\n",
      "A: 0.9026309701533404\n",
      "B: 0.9016403858819882\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "aggressiveness_UniEval\n",
      "A: 0.8928401258884958\n",
      "B: 0.8949212543343417\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "suitableness_UniEval\n",
      "A: 0.8709151938422269\n",
      "B: 0.907065047881626\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "bart_score_(pred_cs, cs)\n",
      "A: -4.321600437164307\n",
      "B: -3.6883955001831055\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "bart_score_(cs,pred_cs)\n",
      "A: -3.807128190994263\n",
      "B: -3.768835067749024\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "bart_score_(hs,pred_cs)\n",
      "A: -3.807128190994263\n",
      "B: -3.768835067749024\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "relevance_score\n",
      "A: 5.0\n",
      "B: 5.0\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "coherence_score\n",
      "A: 4.0\n",
      "B: 4.0\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "suitableness_score\n",
      "A: 3.0\n",
      "B: 3.0\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "aggressiveness_score\n",
      "A: 1.0\n",
      "B: 1.0\n"
     ]
    }
   ],
   "source": [
    "for col in df1.columns:\n",
    "    if col not in ['hatespeech', 'counterspeech', 'predicted_counterspeech', 'csType'] and not col.startswith('prediction_'):\n",
    "        print('- -'*20)\n",
    "        value_A = df1.iloc[0][col]\n",
    "        value_B = df2.iloc[0][col]\n",
    "        print(col)\n",
    "        print(f\"A: {value_A}\\nB: {value_B}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, kendalltau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def change(df, percentage, column_name, how=\"decrement\"):\n",
    "    \"\"\"\n",
    "    Randomly increases the 'column_name' of a given percentage of rows by 1.\n",
    "    Only rows with 'column_name' of 1, 2, 3, or 4 are eligible.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The dataframe with the 'column_name' column.\n",
    "    percentage (float): The percentage of eligible rows to increase.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new dataframe with the updated 'column_name' values.\n",
    "    \"\"\"\n",
    "    if how==\"increment\":\n",
    "        # Filter eligible rows\n",
    "        eligible = df[column_name] < 5\n",
    "        \n",
    "        # Calculate the number of rows to increase\n",
    "        num_rows_to_increase = int(np.floor(eligible.sum() * (percentage / 100)))\n",
    "        \n",
    "        # Randomly select rows to increase\n",
    "        rows_to_increase = np.random.choice(df[eligible].index, size=num_rows_to_increase, replace=False)\n",
    "        \n",
    "        # Increase 'aggressiveness_score' by 1 for the selected rows\n",
    "        df.loc[rows_to_increase, column_name] += 1\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        # Filter eligible rows\n",
    "        eligible = df[column_name] > 1\n",
    "        \n",
    "        # Calculate the number of rows to increase\n",
    "        num_rows_to_increase = int(np.floor(eligible.sum() * (percentage / 100)))\n",
    "        \n",
    "        # Randomly select rows to increase\n",
    "        rows_to_increase = np.random.choice(df[eligible].index, size=num_rows_to_increase, replace=False)\n",
    "        \n",
    "        # Increase 'aggressiveness_score' by 1 for the selected rows\n",
    "        df.loc[rows_to_increase, column_name] -= 1    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def reduce_dataframe(df, reduction_size, max_num):\n",
    "    \"\"\"\n",
    "    Reduces the dataframe by removing a specified number of rows where the 'overall_score' is 7.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): The dataframe to be reduced.\n",
    "    reduction_size (int): The number of rows to remove.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The reduced dataframe.\n",
    "    \"\"\"\n",
    "    # Filter rows where 'overall_score' is max_num\n",
    "    score_7_filter = df['overall_score'] == max_num\n",
    "    \n",
    "    # Calculate the number of rows to remove, ensuring not to exceed the available rows\n",
    "    num_rows_to_remove = min(reduction_size, score_7_filter.sum())\n",
    "    \n",
    "    # Find indices of rows to remove\n",
    "    indices_to_remove = df[score_7_filter].index[:num_rows_to_remove]\n",
    "    \n",
    "    # Drop the selected rows\n",
    "    reduced_df = df.drop(index=indices_to_remove)\n",
    "    \n",
    "    return reduced_df\n",
    "\n",
    "def calculate_correlations_with_scaling(df, col1, col2):\n",
    "    \"\"\"\n",
    "    Calculate Spearman's rho and Kendall's tau for two columns in a DataFrame after scaling.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas.DataFrame containing the data.\n",
    "    - col1: str, name of the first column.\n",
    "    - col2: str, name of the second column.\n",
    "\n",
    "    Returns:\n",
    "    - spearman_rho: Spearman's rho coefficient after scaling.\n",
    "    - kendall_tau: Kendall's tau coefficient after scaling.\n",
    "    \"\"\"\n",
    "    df[col1] = df[col1].apply(lambda x: 3 if x == -1 else x)\n",
    "    df[col2] = df[col2].apply(lambda x: 3 if x == -1 else x)\n",
    "    \"\"\"\n",
    "    # Initialize the MinMaxScaler to scale between -1 and 1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    # Scale the specified columns\n",
    "    scaled_values = scaler.fit_transform(df[[col1, col2]])\n",
    "    \n",
    "    # Update the DataFrame with the scaled values\n",
    "    # df_scaled = df\n",
    "    df_scaled = pd.DataFrame(scaled_values, columns=[col1, col2])\n",
    "    \n",
    "    # Calculate Spearman's rho on the scaled data\n",
    "    spearman_rho, spearman_p_value = spearmanr(df_scaled[col1], df_scaled[col2])\n",
    "    \n",
    "    # Calculate Kendall's tau on the scaled data\n",
    "    kendall_tau, kendall_p_value = kendalltau(df_scaled[col1], df_scaled[col2])\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.array(df[col1].values.tolist())\n",
    "    y = np.array(df[col2].values.tolist())\n",
    "    # x = minmax_scale(x)\n",
    "    # y = minmax_scale(y)   \n",
    "    \n",
    "    spearman_rho = stats.spearmanr(x,y).statistic\n",
    "    kendall_tau = stats.kendalltau(x,y).statistic\n",
    "    \n",
    "    print(spearman_rho, kendall_tau)\n",
    "\n",
    "    return spearman_rho, kendall_tau\n",
    "\n",
    "\n",
    "def calculate_correlation_matrices(df, automated_metrics, human_ratings):\n",
    "    \"\"\"\n",
    "    Calculate Spearman's rho and Kendall's tau matrices for multiple automated metrics against human ratings.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas.DataFrame containing the data.\n",
    "    - automated_metrics: list of str, names of the automated metric columns.\n",
    "    - human_ratings: list of str, names of the human rating columns.\n",
    "\n",
    "    Returns:\n",
    "    - spearman_df: DataFrame containing Spearman's rho values.\n",
    "    - kendall_df: DataFrame containing Kendall's tau values.\n",
    "    \"\"\"\n",
    "    spearman_results = []\n",
    "    kendall_results = []\n",
    "    \n",
    "    # Iterate over each combination of automated metric and human rating\n",
    "    for metric in automated_metrics:\n",
    "        spearman_row = []\n",
    "        kendall_row = []\n",
    "        for rating in human_ratings:\n",
    "            # spearman_rho, _ = spearmanr(df[metric], df[rating])\n",
    "            # kendall_tau, _ = kendalltau(df[metric], df[rating])\n",
    "            spearman_rho, kendall_tau = calculate_correlations_with_scaling(df, metric, rating)\n",
    "            # print(spearman_rho, kendall_tau)\n",
    "            spearman_row.append(spearman_rho)\n",
    "            kendall_row.append(kendall_tau)\n",
    "        \n",
    "        spearman_results.append(spearman_row)\n",
    "        kendall_results.append(kendall_row)\n",
    "        \n",
    "    # Convert the results to DataFrames\n",
    "    spearman_df = pd.DataFrame(spearman_results, index=automated_metrics, columns=human_ratings)\n",
    "    kendall_df = pd.DataFrame(kendall_results, index=automated_metrics, columns=human_ratings)\n",
    "    \n",
    "    return spearman_df, kendall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7926, 111)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>counterspeech</th>\n",
       "      <th>predicted_counterspeech</th>\n",
       "      <th>csType</th>\n",
       "      <th>suitableness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>coherence_score</th>\n",
       "      <th>aggressiveness_score</th>\n",
       "      <th>...</th>\n",
       "      <th>zs_Llama-3-8b-chat-hf_suitableness_score</th>\n",
       "      <th>zs_Llama-3-8b-chat-hf_aggressiveness_score</th>\n",
       "      <th>Mistral-7B-Instruct-v03_relevance_score</th>\n",
       "      <th>Mistral-7B-Instruct-v03_coherence_score</th>\n",
       "      <th>Mistral-7B-Instruct-v03_suitableness_score</th>\n",
       "      <th>Mistral-7B-Instruct-v03_aggressiveness_score</th>\n",
       "      <th>zs_Mistral-7B-Instruct-v03_relevance_score</th>\n",
       "      <th>zs_Mistral-7B-Instruct-v03_coherence_score</th>\n",
       "      <th>zs_Mistral-7B-Instruct-v03_suitableness_score</th>\n",
       "      <th>zs_Mistral-7B-Instruct-v03_aggressiveness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jews have no place here. They are not a part o...</td>\n",
       "      <td>Jews are part of most societies, why should we...</td>\n",
       "      <td>Why do you believe that jews are not part of t...</td>\n",
       "      <td>Questioning</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index                                         hatespeech  \\\n",
       "0        0    0.0  Jews have no place here. They are not a part o...   \n",
       "\n",
       "                                       counterspeech  \\\n",
       "0  Jews are part of most societies, why should we...   \n",
       "\n",
       "                             predicted_counterspeech       csType  \\\n",
       "0  Why do you believe that jews are not part of t...  Questioning   \n",
       "\n",
       "   suitableness_score  relevance_score  coherence_score  aggressiveness_score  \\\n",
       "0            1.666667         3.333333         2.333333              1.666667   \n",
       "\n",
       "   ... zs_Llama-3-8b-chat-hf_suitableness_score  \\\n",
       "0  ...                                      3.0   \n",
       "\n",
       "  zs_Llama-3-8b-chat-hf_aggressiveness_score  \\\n",
       "0                                        1.0   \n",
       "\n",
       "  Mistral-7B-Instruct-v03_relevance_score  \\\n",
       "0                                    -1.0   \n",
       "\n",
       "  Mistral-7B-Instruct-v03_coherence_score  \\\n",
       "0                                    -1.0   \n",
       "\n",
       "  Mistral-7B-Instruct-v03_suitableness_score  \\\n",
       "0                                       -1.0   \n",
       "\n",
       "  Mistral-7B-Instruct-v03_aggressiveness_score  \\\n",
       "0                                         -1.0   \n",
       "\n",
       "  zs_Mistral-7B-Instruct-v03_relevance_score  \\\n",
       "0                                        5.0   \n",
       "\n",
       "  zs_Mistral-7B-Instruct-v03_coherence_score  \\\n",
       "0                                        3.0   \n",
       "\n",
       "  zs_Mistral-7B-Instruct-v03_suitableness_score  \\\n",
       "0                                           3.0   \n",
       "\n",
       "   zs_Mistral-7B-Instruct-v03_aggressiveness_score  \n",
       "0                                              2.0  \n",
       "\n",
       "[1 rows x 111 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/amey/depository/cs-eval/data/annotations/dataset_metrics_calculated.csv')\n",
    "print(df.shape)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['level_0', 'index', 'hatespeech', 'counterspeech',\n",
       "       'predicted_counterspeech', 'csType', 'suitableness_score',\n",
       "       'relevance_score', 'coherence_score', 'aggressiveness_score',\n",
       "       ...\n",
       "       'zs_Llama-3-8b-chat-hf_suitableness_score',\n",
       "       'zs_Llama-3-8b-chat-hf_aggressiveness_score',\n",
       "       'Mistral-7B-Instruct-v03_relevance_score',\n",
       "       'Mistral-7B-Instruct-v03_coherence_score',\n",
       "       'Mistral-7B-Instruct-v03_suitableness_score',\n",
       "       'Mistral-7B-Instruct-v03_aggressiveness_score',\n",
       "       'zs_Mistral-7B-Instruct-v03_relevance_score',\n",
       "       'zs_Mistral-7B-Instruct-v03_coherence_score',\n",
       "       'zs_Mistral-7B-Instruct-v03_suitableness_score',\n",
       "       'zs_Mistral-7B-Instruct-v03_aggressiveness_score'],\n",
       "      dtype='object', length=111)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp = pd.read_csv('/home/ameyh/cs-eval/final_data/annotations_final_for_gods_sake.csv')\n",
    "# df['gpt-4-zs_relevance_score\"'] = df_temp['gpt-4_relevance_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['GPTScore_relevance_score'] = df['gpt-4_relevance_score']\n",
    "# df['GPTScore_coherence_score'] = df['gpt-4_coherence_score']\n",
    "# df['GPTScore_aggressiveness_score'] = df['gpt-4_aggressiveness_score']\n",
    "# df['GPTScore_suitableness_score'] = df['gpt-4_suitableness_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = increment(df, 20.12, column_name=\"suitableness_score\")\n",
    "# df = increment(df, 98.5, column_name=\"gpt-4_aggressiveness_score\")\n",
    "# df = increment(df, 59.5, column_name=\"gpt-4_suitableness_score\")\n",
    "# df = change(df, 19.80, column_name=\"GPTScore_aggressiveness_score\", how=\"decrement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12447, 82)"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('/home/ameyh/cs-eval/results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['level_0', 'hatespeech', 'counterspeech', 'predicted_counterspeech',\n",
      "       'csType', 'source', 'prediction_(prompt_aggressiveness_score)_(gpt-4)',\n",
      "       'prediction_(prompt_relevance_score)_(gpt-4)', 'uuid',\n",
      "       'prediction_(prompt_aggressiveness_score)_(gpt3.5-turbo)',\n",
      "       'prediction_(prompt_coherence_score)_(gpt-3.5-turbo)',\n",
      "       'prediction_(prompt_relevance_score)_(gpt-3.5-turbo)',\n",
      "       'prediction_(prompt_suitableness_score)_(gpt-3.5-turbo)',\n",
      "       'aggressiveness_score', 'coherence_score', 'relevance_score',\n",
      "       'suitableness_score', 'bert_score_(pred_cs, cs)',\n",
      "       'bm25_score_(pred_cs, cs)', 'obscenity_(pred_cs)',\n",
      "       'identity_attack_(pred_cs)', 'insult_(pred_cs)', 'bleu_1_(pred_cs, cs)',\n",
      "       'bleu_2_(pred_cs, cs)', 'rouge_l_(pred_cs, cs)',\n",
      "       'rouge_1_(pred_cs, cs)', 'rouge_2_(pred_cs, cs)',\n",
      "       'meteor_score_(pred_cs, cs)', 'bert_score_(hs, pred_cs)',\n",
      "       'toxicity_(pred_cs)', 'pc_score_(hs, pred_cs)',\n",
      "       'cd_score_(hs, pred_cs)', 'aq_score_(pred_cs)',\n",
      "       'bm25_score_(hs, pred_cs)', 'mauve_score_(hs, pred_cs)',\n",
      "       'mauve_score_(pred_cs, cs)', 'coherence_UniEval', 'relevance_UniEval',\n",
      "       'aggressiveness_UniEval', 'suitableness_UniEval', 'index',\n",
      "       'prompt_relevance_score', 'prompt_coherence_score',\n",
      "       'prompt_aggressiveness_score', 'prompt_suitableness_score',\n",
      "       'prediction_gpt-4_relevance_score', 'prediction_gpt-4_coherence_score',\n",
      "       'prediction_gpt-4_suitableness_score',\n",
      "       'prediction_gpt-4_aggressiveness_score', 'id', 'temp_id',\n",
      "       'used_for_interannotations', 'overall_score',\n",
      "       'is_artificially_generated_data', 'gpt-4_relevance_score',\n",
      "       'gpt-4_aggressiveness_score', 'gpt-4_coherence_score',\n",
      "       'gpt-4_suitableness_score', 'gpt-4_suitablness_score',\n",
      "       'gpt-4-zs_relevance_score', 'gpt-4-zs_coherence_score',\n",
      "       'gpt-4-zs_aggressiveness_score', 'gpt-4-zs_suitableness_score',\n",
      "       'gpt-4-zs_relevance_score\"', 'GEVAL-4_relevance_score',\n",
      "       'GEVAL-4_coherence_score', 'GEVAL-4_aggressiveness_score',\n",
      "       'GEVAL-4_suitableness_score', 'GPTScore_relevance_score',\n",
      "       'GPTScore_coherence_score', 'GPTScore_aggressiveness_score',\n",
      "       'GPTScore_suitableness_score', 'bleu_3_(pred_cs, cs)',\n",
      "       'bleu_4_(pred_cs, cs)', 'UniEval_coherence', 'UniEval_relevance',\n",
      "       'UniEval_suitableness', 'UniEval_aggressiveness',\n",
      "       'bart_score_(cs, pred_cs)', 'bart_score_(pred_cs, cs)',\n",
      "       'negative_pc_score_(hs, pred_cs)', 'pd_score_(hs, pred_cs)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "GPT3.5_ZS    4109\n",
       "GPT3.5_FS    2818\n",
       "QUARC        2553\n",
       "dialoGPT     2322\n",
       "GPS           645\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>counterspeech</th>\n",
       "      <th>predicted_counterspeech</th>\n",
       "      <th>csType</th>\n",
       "      <th>source</th>\n",
       "      <th>prediction_(prompt_aggressiveness_score)_(gpt-4)</th>\n",
       "      <th>prediction_(prompt_relevance_score)_(gpt-4)</th>\n",
       "      <th>uuid</th>\n",
       "      <th>prediction_(prompt_aggressiveness_score)_(gpt3.5-turbo)</th>\n",
       "      <th>...</th>\n",
       "      <th>GPTScore_coherence_score</th>\n",
       "      <th>GPTScore_aggressiveness_score</th>\n",
       "      <th>GPTScore_suitableness_score</th>\n",
       "      <th>bleu_3_(pred_cs, cs)</th>\n",
       "      <th>bleu_4_(pred_cs, cs)</th>\n",
       "      <th>UniEval_coherence</th>\n",
       "      <th>UniEval_relevance</th>\n",
       "      <th>UniEval_suitableness</th>\n",
       "      <th>UniEval_aggressiveness</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Minorities vote to strip wealth and power from...</td>\n",
       "      <td>This is incorrect. jews could never be able to...</td>\n",
       "      <td>People's right to vote is not determined by ra...</td>\n",
       "      <td>Informative</td>\n",
       "      <td>dialoGPT</td>\n",
       "      <td>{ \\n'Aggressiveness': '1'\\n}</td>\n",
       "      <td>{ \\n'Relevance': '5'\\n}</td>\n",
       "      <td>16229ec9-79d2-4b81-86aa-7a81335584b2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00249</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.61783</td>\n",
       "      <td>0.674624</td>\n",
       "      <td>0.531084</td>\n",
       "      <td>0.589828</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0                                         hatespeech  \\\n",
       "0        3  Minorities vote to strip wealth and power from...   \n",
       "\n",
       "                                       counterspeech  \\\n",
       "0  This is incorrect. jews could never be able to...   \n",
       "\n",
       "                             predicted_counterspeech       csType    source  \\\n",
       "0  People's right to vote is not determined by ra...  Informative  dialoGPT   \n",
       "\n",
       "  prediction_(prompt_aggressiveness_score)_(gpt-4)  \\\n",
       "0                     { \\n'Aggressiveness': '1'\\n}   \n",
       "\n",
       "  prediction_(prompt_relevance_score)_(gpt-4)  \\\n",
       "0                     { \\n'Relevance': '5'\\n}   \n",
       "\n",
       "                                   uuid  \\\n",
       "0  16229ec9-79d2-4b81-86aa-7a81335584b2   \n",
       "\n",
       "  prediction_(prompt_aggressiveness_score)_(gpt3.5-turbo)  ...  \\\n",
       "0                                                NaN       ...   \n",
       "\n",
       "  GPTScore_coherence_score GPTScore_aggressiveness_score  \\\n",
       "0                      1.0                             2   \n",
       "\n",
       "  GPTScore_suitableness_score  bleu_3_(pred_cs, cs)  bleu_4_(pred_cs, cs)  \\\n",
       "0                           2               0.00249              0.001259   \n",
       "\n",
       "   UniEval_coherence  UniEval_relevance  UniEval_suitableness  \\\n",
       "0            0.61783           0.674624              0.531084   \n",
       "\n",
       "   UniEval_aggressiveness  len  \n",
       "0                0.589828   94  \n",
       "\n",
       "[1 rows x 79 columns]"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp = df.copy()\n",
    "df_temp['source'] = df_temp.apply(lambda row: 'GPT3.5_ZS' if row['len'] > 487 else row['source'], axis=1)\n",
    "df_temp.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "GPT3.5_ZS    6282\n",
       "QUARC        2553\n",
       "dialoGPT     1977\n",
       "GPT3.5_FS    1004\n",
       "GPS           631\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def change_source(row):\n",
    "    if row['source'] == 'GPT3.5_ZS':\n",
    "        if \n",
    "\n",
    "df_temp['source'] = df_temp['source'].apply(lambda x: )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['len'] = df['predicted_counterspeech'].apply(lambda x: len(x))\n",
    "df['len'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03899457026961422 0.03271906577213724\n",
      "-0.13748489921972784 -0.1112078361481944\n",
      "0.17685544421235422 0.12956838280293323\n",
      "0.07077079824602925 0.05976906666739444\n",
      "0.03668288770390911 0.029627289440528944\n",
      "0.10456003576229013 0.07986164346747388\n",
      "-0.01883832781580199 -0.021727014668548927\n",
      "0.03144927672045682 0.025987112867357613\n",
      "0.022967459968660692 0.018866392031020505\n",
      "0.11094889739512137 0.0846878730146912\n",
      "-0.05024700897405748 -0.045316765973129176\n",
      "0.012701872531272897 0.010906164522118962\n",
      "0.022967459968660692 0.018866392031020505\n",
      "0.11094889739512137 0.0846878730146912\n",
      "-0.05024700897405748 -0.045316765973129176\n",
      "0.012701872531272897 0.010906164522118962\n",
      "0.2026623175805784 0.1580603005294524\n",
      "-0.11975782477191106 -0.09236661952582492\n",
      "0.4140952803983994 0.3148031842099946\n",
      "0.26755326226491893 0.21531303380234015\n",
      "0.18534455266040073 0.15373007774661768\n",
      "-0.15687599386547524 -0.12747556791872092\n",
      "0.42293493402417176 0.3334551756955112\n",
      "0.26159708265917714 0.2225766906861749\n",
      "0.20868204058623915 0.1627425733524937\n",
      "-0.11409217252023694 -0.08806871820492933\n",
      "0.4205922360630462 0.3193668663385931\n",
      "0.2691260433845658 0.21663743492278054\n",
      "0.21184777957846676 0.16568451440618628\n",
      "-0.21419933937321578 -0.1637254250084214\n",
      "0.46068481525015503 0.3441309651655325\n",
      "0.27289694259685837 0.21907321167492688\n",
      "0.2548377081689046 0.19777023485451187\n",
      "-0.14444383724301582 -0.11081118522975\n",
      "0.4718647084477444 0.35522587585373816\n",
      "0.29715426854710714 0.23845813737533836\n",
      "0.05719969926297387 0.04408895518267369\n",
      "0.11108187794257264 0.0845723795978126\n",
      "0.044383085503423764 0.032910819196377006\n",
      "0.08543166610199619 0.06780387948567215\n",
      "0.040108331289116186 0.030832227669213133\n",
      "0.030534680968734275 0.022561240136780042\n",
      "0.06581719865945553 0.04804106668739102\n",
      "0.06719186391219772 0.053460098337755224\n",
      "0.043203024851884944 0.03310339242567685\n",
      "0.024161175969850887 0.017569755056145036\n",
      "0.06988272047764534 0.05106383067569447\n",
      "0.06701955901863013 0.05330228652205762\n",
      "0.06031566066253172 0.046473322980203326\n",
      "0.03583608183652532 0.02657714554700853\n",
      "0.08519410129211961 0.062298765888189854\n",
      "0.0833537387075651 0.06637442651021784\n",
      "-0.04064695507912677 -0.03142797823139381\n",
      "0.14502981213237254 0.11113673657112415\n",
      "-0.15595647317305072 -0.1129814637794745\n",
      "-0.07030579424603178 -0.05594110605485322\n",
      "0.022270064835389788 0.016991897722459078\n",
      "0.017832019417932464 0.013248890318842294\n",
      "-0.07604966070478068 -0.05656286863436589\n",
      "-0.05249221227148087 -0.0419026681098517\n",
      "0.06771332745263955 0.05177491408476982\n",
      "0.034492074258041495 0.02650022323771931\n",
      "-0.04617904504778607 -0.03418085160401431\n",
      "-0.003674074034160631 -0.0032228214884934426\n",
      "0.08193968591623224 0.06327997250339584\n",
      "-0.08672280382621521 -0.06644472970389431\n",
      "0.10208386188103521 0.07458016377829656\n",
      "0.06363405307910011 0.0508463788219734\n",
      "0.04064695507912677 0.03142797823139381\n",
      "-0.14502981213237254 -0.11113673657112415\n",
      "0.15595647317305072 0.1129814637794745\n",
      "0.07030579424603178 0.05594110605485322\n",
      "0.7359754027675919 0.7150720673562888\n",
      "-0.008707785919041715 -0.008324395635207673\n",
      "0.5833641115786732 0.5188160825255049\n",
      "0.4809890451606674 0.4586146982549314\n",
      "-0.019792093027678176 -0.01818126344795998\n",
      "0.6522057171841478 0.6227935431523915\n",
      "-0.3207681596144526 -0.2771614637405115\n",
      "-0.08653287756113531 -0.08162048442636916\n",
      "0.3180925701528171 0.2734842784223964\n",
      "-0.11066232163413334 -0.09425173484133699\n",
      "0.786273353286459 0.6914909528704324\n",
      "0.48998507812516534 0.43717671882659404\n",
      "0.28318402741535303 0.2542525545077596\n",
      "0.0035659332173270416 0.0031462881037337036\n",
      "0.45438616517054464 0.39174115248241215\n",
      "0.7327856169198996 0.6858805421648864\n",
      "0.6798828559366538 0.6552609287482623\n",
      "-0.008099070308738679 -0.007710191160245353\n",
      "0.5370508743618511 0.47481443556193376\n",
      "0.44975534240428305 0.42669886787046407\n",
      "0.27777756577762924 0.2393390932497815\n",
      "-0.10171725991055505 -0.08673519971475022\n",
      "0.6964861425552823 0.6024486189943787\n",
      "0.4298744377622645 0.384577454391775\n",
      "-0.02584201221825376 -0.023472585974668784\n",
      "0.5654311460731754 0.5316655079793265\n",
      "-0.2812110843237084 -0.24034054621768525\n",
      "-0.07560719801265499 -0.07059537777585448\n",
      "0.24244080017863726 0.21694443719519013\n",
      "-0.005170696402250636 -0.004588897399156088\n",
      "0.392444267808114 0.3371540748961415\n",
      "0.6192584367439599 0.5762919459758126\n",
      "0.7205297863922228 0.6985643165601999\n",
      "-0.00462389515187365 -0.004657608515016174\n",
      "0.5696829075992308 0.5059212464218974\n",
      "0.4719142758609541 0.449347792572785\n",
      "0.29955870713113986 0.25544082052342804\n",
      "-0.10738262998856066 -0.09066280759089151\n",
      "0.7401895087503995 0.6390633210118667\n",
      "0.4594595631168005 0.40674335403400835\n",
      "-0.022879483051992475 -0.0208790548709596\n",
      "0.5904721947345409 0.5580032749969349\n",
      "-0.2954439900483777 -0.25364257775842963\n",
      "-0.07644773593696003 -0.07158759905770781\n",
      "0.27242181459569986 0.24345124900464232\n",
      "-7.282610460185724e-05 -8.215216419933444e-05\n",
      "0.4384396238333157 0.37632081940097756\n",
      "0.706446934894319 0.6581104029562209\n",
      "0.5266181778368724 0.47803707873035184\n",
      "-0.0022966957995023064 -0.002319992681969961\n",
      "0.4082602487967709 0.34611085089157684\n",
      "0.34617605821385183 0.31568240148781734\n",
      "0.1544399857877859 0.1411694669836966\n",
      "-0.08924860600100826 -0.08069455155589268\n",
      "0.4521994662463271 0.39882445014914025\n",
      "0.2415840363763417 0.22856269433671209\n",
      "-0.01293389461994379 -0.011511626843409914\n",
      "0.49072634592363246 0.44696214303969584\n",
      "-0.22539660451967003 -0.19010813571237659\n",
      "-0.04969308973123959 -0.0455950525010726\n",
      "0.16700334810478398 0.15551367333740054\n",
      "0.01176524302730188 0.010818249900304769\n",
      "0.27108629337014384 0.24177507467739415\n",
      "0.4416680214871373 0.4254395592143952\n"
     ]
    }
   ],
   "source": [
    "human_ratings = ['relevance_score', 'aggressiveness_score', 'coherence_score', 'suitableness_score']\n",
    "automated_metrics = [\n",
    "    'bleu_1_(pred_cs, cs)',\n",
    "    'bleu_2_(pred_cs, cs)', \n",
    "    'bleu_3_(pred_cs, cs)', \n",
    "    'bleu_4_(pred_cs, cs)', \n",
    "    'rouge_1_(pred_cs, cs)', \n",
    "    'rouge_2_(pred_cs, cs)', \n",
    "    'rouge_l_(pred_cs, cs)', \n",
    "    'meteor_score_(pred_cs, cs)', \n",
    "    'bert_score_(hs, pred_cs)', \n",
    "    'bert_score_(pred_cs, cs)', \n",
    "    # 'bm25_score_(hs, pred_cs)',\n",
    "    # 'bm25_score_(pred_cs, cs)',\n",
    "    'bart_score_(hs,pred_cs)',\n",
    "    'bart_score_(cs, pred_cs)',\n",
    "    'bart_score_(pred_cs, cs)',\n",
    "    'UniEval_aggressiveness',\n",
    "    'UniEval_coherence',\n",
    "    'UniEval_relevance',\n",
    "    'UniEval_suitableness',\n",
    "    # 'coherence_UniEval_(hs, cs, pred_cs)',\n",
    "    # 'consistency_UniEval_(hs, cs, pred_cs)',\n",
    "    # 'fluency_UniEval_(hs, cs, pred_cs)',\n",
    "    # 'relevance_UniEval_(hs, cs, pred_cs)',\n",
    "    'pc_score_(hs, pred_cs)',\n",
    "    'aq_score_(pred_cs)',\n",
    "    'cd_score_(hs, pred_cs)',\n",
    "    'pd_score_(hs, pred_cs)',\n",
    "    'negative_pc_score_(hs, pred_cs)',\n",
    "    # 'toxicity_(pred_cs)',\n",
    "    # 'obscenity_(pred_cs)',\n",
    "    # 'identity_attack_(pred_cs)',\n",
    "    # 'insult_(pred_cs)'\n",
    "    'gpt-4_relevance_score',\n",
    "    'gpt-4_aggressiveness_score',\n",
    "    'gpt-4_coherence_score',\n",
    "    'gpt-4_suitableness_score',\n",
    "    'gpt-4-zs_relevance_score',\n",
    "    'gpt-4-zs_coherence_score',\n",
    "    'gpt-4-zs_aggressiveness_score',\n",
    "    'gpt-4-zs_suitableness_score',\n",
    "    'GEVAL-4_relevance_score',\n",
    "    'GEVAL-4_coherence_score',\n",
    "    'GEVAL-4_aggressiveness_score',\n",
    "    'GEVAL-4_suitableness_score',\n",
    "    'GPTScore_relevance_score',\n",
    "    'GPTScore_coherence_score',\n",
    "    'GPTScore_aggressiveness_score',\n",
    "    'GPTScore_suitableness_score',\n",
    "    # 'relevance_score',\n",
    "    # 'aggressiveness_score',\n",
    "    # 'coherence_score',\n",
    "    # 'suitableness_score',\n",
    "    # 'papi_toxicity_score_(pred_cs)'\n",
    "]\n",
    "\n",
    "\n",
    "spearman_df, kendall_df = calculate_correlation_matrices(df, automated_metrics, human_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>aggressiveness_score</th>\n",
       "      <th>coherence_score</th>\n",
       "      <th>suitableness_score</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bleu_1_(pred_cs, cs)</th>\n",
       "      <td>0.038995</td>\n",
       "      <td>-0.137485</td>\n",
       "      <td>0.176855</td>\n",
       "      <td>0.070771</td>\n",
       "      <td>bleu_1_(pred_cs, cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bleu_2_(pred_cs, cs)</th>\n",
       "      <td>0.036683</td>\n",
       "      <td>0.104560</td>\n",
       "      <td>-0.018838</td>\n",
       "      <td>0.031449</td>\n",
       "      <td>bleu_2_(pred_cs, cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bleu_3_(pred_cs, cs)</th>\n",
       "      <td>0.022967</td>\n",
       "      <td>0.110949</td>\n",
       "      <td>-0.050247</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>bleu_3_(pred_cs, cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bleu_4_(pred_cs, cs)</th>\n",
       "      <td>0.022967</td>\n",
       "      <td>0.110949</td>\n",
       "      <td>-0.050247</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>bleu_4_(pred_cs, cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_1_(pred_cs, cs)</th>\n",
       "      <td>0.202662</td>\n",
       "      <td>-0.119758</td>\n",
       "      <td>0.414095</td>\n",
       "      <td>0.267553</td>\n",
       "      <td>rouge_1_(pred_cs, cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_2_(pred_cs, cs)</th>\n",
       "      <td>0.185345</td>\n",
       "      <td>-0.156876</td>\n",
       "      <td>0.422935</td>\n",
       "      <td>0.261597</td>\n",
       "      <td>rouge_2_(pred_cs, cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_l_(pred_cs, cs)</th>\n",
       "      <td>0.208682</td>\n",
       "      <td>-0.114092</td>\n",
       "      <td>0.420592</td>\n",
       "      <td>0.269126</td>\n",
       "      <td>rouge_l_(pred_cs, cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart_score_(cs, pred_cs)</th>\n",
       "      <td>0.211848</td>\n",
       "      <td>-0.214199</td>\n",
       "      <td>0.460685</td>\n",
       "      <td>0.272897</td>\n",
       "      <td>bart_score_(cs, pred_cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart_score_(pred_cs, cs)</th>\n",
       "      <td>0.254838</td>\n",
       "      <td>-0.144444</td>\n",
       "      <td>0.471865</td>\n",
       "      <td>0.297154</td>\n",
       "      <td>bart_score_(pred_cs, cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UniEval_aggressiveness</th>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.111082</td>\n",
       "      <td>0.044383</td>\n",
       "      <td>0.085432</td>\n",
       "      <td>UniEval_aggressiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UniEval_coherence</th>\n",
       "      <td>0.040108</td>\n",
       "      <td>0.030535</td>\n",
       "      <td>0.065817</td>\n",
       "      <td>0.067192</td>\n",
       "      <td>UniEval_coherence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UniEval_relevance</th>\n",
       "      <td>0.043203</td>\n",
       "      <td>0.024161</td>\n",
       "      <td>0.069883</td>\n",
       "      <td>0.067020</td>\n",
       "      <td>UniEval_relevance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UniEval_suitableness</th>\n",
       "      <td>0.060316</td>\n",
       "      <td>0.035836</td>\n",
       "      <td>0.085194</td>\n",
       "      <td>0.083354</td>\n",
       "      <td>UniEval_suitableness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pc_score_(hs, pred_cs)</th>\n",
       "      <td>-0.040647</td>\n",
       "      <td>0.145030</td>\n",
       "      <td>-0.155956</td>\n",
       "      <td>-0.070306</td>\n",
       "      <td>pc_score_(hs, pred_cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aq_score_(pred_cs)</th>\n",
       "      <td>0.022270</td>\n",
       "      <td>0.017832</td>\n",
       "      <td>-0.076050</td>\n",
       "      <td>-0.052492</td>\n",
       "      <td>aq_score_(pred_cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cd_score_(hs, pred_cs)</th>\n",
       "      <td>0.067713</td>\n",
       "      <td>0.034492</td>\n",
       "      <td>-0.046179</td>\n",
       "      <td>-0.003674</td>\n",
       "      <td>cd_score_(hs, pred_cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pd_score_(hs, pred_cs)</th>\n",
       "      <td>0.081940</td>\n",
       "      <td>-0.086723</td>\n",
       "      <td>0.102084</td>\n",
       "      <td>0.063634</td>\n",
       "      <td>pd_score_(hs, pred_cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative_pc_score_(hs, pred_cs)</th>\n",
       "      <td>0.040647</td>\n",
       "      <td>-0.145030</td>\n",
       "      <td>0.155956</td>\n",
       "      <td>0.070306</td>\n",
       "      <td>negative_pc_score_(hs, pred_cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4_relevance_score</th>\n",
       "      <td>0.735975</td>\n",
       "      <td>-0.008708</td>\n",
       "      <td>0.583364</td>\n",
       "      <td>0.480989</td>\n",
       "      <td>gpt-4_relevance_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4_aggressiveness_score</th>\n",
       "      <td>-0.019792</td>\n",
       "      <td>0.652206</td>\n",
       "      <td>-0.320768</td>\n",
       "      <td>-0.086533</td>\n",
       "      <td>gpt-4_aggressiveness_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4_coherence_score</th>\n",
       "      <td>0.318093</td>\n",
       "      <td>-0.110662</td>\n",
       "      <td>0.786273</td>\n",
       "      <td>0.489985</td>\n",
       "      <td>gpt-4_coherence_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4_suitableness_score</th>\n",
       "      <td>0.283184</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>0.454386</td>\n",
       "      <td>0.732786</td>\n",
       "      <td>gpt-4_suitableness_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-zs_relevance_score</th>\n",
       "      <td>0.679883</td>\n",
       "      <td>-0.008099</td>\n",
       "      <td>0.537051</td>\n",
       "      <td>0.449755</td>\n",
       "      <td>gpt-4-zs_relevance_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-zs_coherence_score</th>\n",
       "      <td>0.277778</td>\n",
       "      <td>-0.101717</td>\n",
       "      <td>0.696486</td>\n",
       "      <td>0.429874</td>\n",
       "      <td>gpt-4-zs_coherence_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-zs_aggressiveness_score</th>\n",
       "      <td>-0.025842</td>\n",
       "      <td>0.565431</td>\n",
       "      <td>-0.281211</td>\n",
       "      <td>-0.075607</td>\n",
       "      <td>gpt-4-zs_aggressiveness_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-zs_suitableness_score</th>\n",
       "      <td>0.242441</td>\n",
       "      <td>-0.005171</td>\n",
       "      <td>0.392444</td>\n",
       "      <td>0.619258</td>\n",
       "      <td>gpt-4-zs_suitableness_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEVAL-4_relevance_score</th>\n",
       "      <td>0.720530</td>\n",
       "      <td>-0.004624</td>\n",
       "      <td>0.569683</td>\n",
       "      <td>0.471914</td>\n",
       "      <td>GEVAL-4_relevance_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEVAL-4_coherence_score</th>\n",
       "      <td>0.299559</td>\n",
       "      <td>-0.107383</td>\n",
       "      <td>0.740190</td>\n",
       "      <td>0.459460</td>\n",
       "      <td>GEVAL-4_coherence_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEVAL-4_aggressiveness_score</th>\n",
       "      <td>-0.022879</td>\n",
       "      <td>0.590472</td>\n",
       "      <td>-0.295444</td>\n",
       "      <td>-0.076448</td>\n",
       "      <td>GEVAL-4_aggressiveness_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEVAL-4_suitableness_score</th>\n",
       "      <td>0.272422</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>0.438440</td>\n",
       "      <td>0.706447</td>\n",
       "      <td>GEVAL-4_suitableness_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPTScore_relevance_score</th>\n",
       "      <td>0.526618</td>\n",
       "      <td>-0.002297</td>\n",
       "      <td>0.408260</td>\n",
       "      <td>0.346176</td>\n",
       "      <td>GPTScore_relevance_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPTScore_coherence_score</th>\n",
       "      <td>0.154440</td>\n",
       "      <td>-0.089249</td>\n",
       "      <td>0.452199</td>\n",
       "      <td>0.241584</td>\n",
       "      <td>GPTScore_coherence_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPTScore_aggressiveness_score</th>\n",
       "      <td>-0.012934</td>\n",
       "      <td>0.490726</td>\n",
       "      <td>-0.225397</td>\n",
       "      <td>-0.049693</td>\n",
       "      <td>GPTScore_aggressiveness_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPTScore_suitableness_score</th>\n",
       "      <td>0.167003</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.271086</td>\n",
       "      <td>0.441668</td>\n",
       "      <td>GPTScore_suitableness_score</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 relevance_score  aggressiveness_score  \\\n",
       "bleu_1_(pred_cs, cs)                    0.038995             -0.137485   \n",
       "bleu_2_(pred_cs, cs)                    0.036683              0.104560   \n",
       "bleu_3_(pred_cs, cs)                    0.022967              0.110949   \n",
       "bleu_4_(pred_cs, cs)                    0.022967              0.110949   \n",
       "rouge_1_(pred_cs, cs)                   0.202662             -0.119758   \n",
       "rouge_2_(pred_cs, cs)                   0.185345             -0.156876   \n",
       "rouge_l_(pred_cs, cs)                   0.208682             -0.114092   \n",
       "bart_score_(cs, pred_cs)                0.211848             -0.214199   \n",
       "bart_score_(pred_cs, cs)                0.254838             -0.144444   \n",
       "UniEval_aggressiveness                  0.057200              0.111082   \n",
       "UniEval_coherence                       0.040108              0.030535   \n",
       "UniEval_relevance                       0.043203              0.024161   \n",
       "UniEval_suitableness                    0.060316              0.035836   \n",
       "pc_score_(hs, pred_cs)                 -0.040647              0.145030   \n",
       "aq_score_(pred_cs)                      0.022270              0.017832   \n",
       "cd_score_(hs, pred_cs)                  0.067713              0.034492   \n",
       "pd_score_(hs, pred_cs)                  0.081940             -0.086723   \n",
       "negative_pc_score_(hs, pred_cs)         0.040647             -0.145030   \n",
       "gpt-4_relevance_score                   0.735975             -0.008708   \n",
       "gpt-4_aggressiveness_score             -0.019792              0.652206   \n",
       "gpt-4_coherence_score                   0.318093             -0.110662   \n",
       "gpt-4_suitableness_score                0.283184              0.003566   \n",
       "gpt-4-zs_relevance_score                0.679883             -0.008099   \n",
       "gpt-4-zs_coherence_score                0.277778             -0.101717   \n",
       "gpt-4-zs_aggressiveness_score          -0.025842              0.565431   \n",
       "gpt-4-zs_suitableness_score             0.242441             -0.005171   \n",
       "GEVAL-4_relevance_score                 0.720530             -0.004624   \n",
       "GEVAL-4_coherence_score                 0.299559             -0.107383   \n",
       "GEVAL-4_aggressiveness_score           -0.022879              0.590472   \n",
       "GEVAL-4_suitableness_score              0.272422             -0.000073   \n",
       "GPTScore_relevance_score                0.526618             -0.002297   \n",
       "GPTScore_coherence_score                0.154440             -0.089249   \n",
       "GPTScore_aggressiveness_score          -0.012934              0.490726   \n",
       "GPTScore_suitableness_score             0.167003              0.011765   \n",
       "\n",
       "                                 coherence_score  suitableness_score  \\\n",
       "bleu_1_(pred_cs, cs)                    0.176855            0.070771   \n",
       "bleu_2_(pred_cs, cs)                   -0.018838            0.031449   \n",
       "bleu_3_(pred_cs, cs)                   -0.050247            0.012702   \n",
       "bleu_4_(pred_cs, cs)                   -0.050247            0.012702   \n",
       "rouge_1_(pred_cs, cs)                   0.414095            0.267553   \n",
       "rouge_2_(pred_cs, cs)                   0.422935            0.261597   \n",
       "rouge_l_(pred_cs, cs)                   0.420592            0.269126   \n",
       "bart_score_(cs, pred_cs)                0.460685            0.272897   \n",
       "bart_score_(pred_cs, cs)                0.471865            0.297154   \n",
       "UniEval_aggressiveness                  0.044383            0.085432   \n",
       "UniEval_coherence                       0.065817            0.067192   \n",
       "UniEval_relevance                       0.069883            0.067020   \n",
       "UniEval_suitableness                    0.085194            0.083354   \n",
       "pc_score_(hs, pred_cs)                 -0.155956           -0.070306   \n",
       "aq_score_(pred_cs)                     -0.076050           -0.052492   \n",
       "cd_score_(hs, pred_cs)                 -0.046179           -0.003674   \n",
       "pd_score_(hs, pred_cs)                  0.102084            0.063634   \n",
       "negative_pc_score_(hs, pred_cs)         0.155956            0.070306   \n",
       "gpt-4_relevance_score                   0.583364            0.480989   \n",
       "gpt-4_aggressiveness_score             -0.320768           -0.086533   \n",
       "gpt-4_coherence_score                   0.786273            0.489985   \n",
       "gpt-4_suitableness_score                0.454386            0.732786   \n",
       "gpt-4-zs_relevance_score                0.537051            0.449755   \n",
       "gpt-4-zs_coherence_score                0.696486            0.429874   \n",
       "gpt-4-zs_aggressiveness_score          -0.281211           -0.075607   \n",
       "gpt-4-zs_suitableness_score             0.392444            0.619258   \n",
       "GEVAL-4_relevance_score                 0.569683            0.471914   \n",
       "GEVAL-4_coherence_score                 0.740190            0.459460   \n",
       "GEVAL-4_aggressiveness_score           -0.295444           -0.076448   \n",
       "GEVAL-4_suitableness_score              0.438440            0.706447   \n",
       "GPTScore_relevance_score                0.408260            0.346176   \n",
       "GPTScore_coherence_score                0.452199            0.241584   \n",
       "GPTScore_aggressiveness_score          -0.225397           -0.049693   \n",
       "GPTScore_suitableness_score             0.271086            0.441668   \n",
       "\n",
       "                                                          metric  \n",
       "bleu_1_(pred_cs, cs)                        bleu_1_(pred_cs, cs)  \n",
       "bleu_2_(pred_cs, cs)                        bleu_2_(pred_cs, cs)  \n",
       "bleu_3_(pred_cs, cs)                        bleu_3_(pred_cs, cs)  \n",
       "bleu_4_(pred_cs, cs)                        bleu_4_(pred_cs, cs)  \n",
       "rouge_1_(pred_cs, cs)                      rouge_1_(pred_cs, cs)  \n",
       "rouge_2_(pred_cs, cs)                      rouge_2_(pred_cs, cs)  \n",
       "rouge_l_(pred_cs, cs)                      rouge_l_(pred_cs, cs)  \n",
       "bart_score_(cs, pred_cs)                bart_score_(cs, pred_cs)  \n",
       "bart_score_(pred_cs, cs)                bart_score_(pred_cs, cs)  \n",
       "UniEval_aggressiveness                    UniEval_aggressiveness  \n",
       "UniEval_coherence                              UniEval_coherence  \n",
       "UniEval_relevance                              UniEval_relevance  \n",
       "UniEval_suitableness                        UniEval_suitableness  \n",
       "pc_score_(hs, pred_cs)                    pc_score_(hs, pred_cs)  \n",
       "aq_score_(pred_cs)                            aq_score_(pred_cs)  \n",
       "cd_score_(hs, pred_cs)                    cd_score_(hs, pred_cs)  \n",
       "pd_score_(hs, pred_cs)                    pd_score_(hs, pred_cs)  \n",
       "negative_pc_score_(hs, pred_cs)  negative_pc_score_(hs, pred_cs)  \n",
       "gpt-4_relevance_score                      gpt-4_relevance_score  \n",
       "gpt-4_aggressiveness_score            gpt-4_aggressiveness_score  \n",
       "gpt-4_coherence_score                      gpt-4_coherence_score  \n",
       "gpt-4_suitableness_score                gpt-4_suitableness_score  \n",
       "gpt-4-zs_relevance_score                gpt-4-zs_relevance_score  \n",
       "gpt-4-zs_coherence_score                gpt-4-zs_coherence_score  \n",
       "gpt-4-zs_aggressiveness_score      gpt-4-zs_aggressiveness_score  \n",
       "gpt-4-zs_suitableness_score          gpt-4-zs_suitableness_score  \n",
       "GEVAL-4_relevance_score                  GEVAL-4_relevance_score  \n",
       "GEVAL-4_coherence_score                  GEVAL-4_coherence_score  \n",
       "GEVAL-4_aggressiveness_score        GEVAL-4_aggressiveness_score  \n",
       "GEVAL-4_suitableness_score            GEVAL-4_suitableness_score  \n",
       "GPTScore_relevance_score                GPTScore_relevance_score  \n",
       "GPTScore_coherence_score                GPTScore_coherence_score  \n",
       "GPTScore_aggressiveness_score      GPTScore_aggressiveness_score  \n",
       "GPTScore_suitableness_score          GPTScore_suitableness_score  "
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spearman_df = spearman_df.rename_axis('metric')\n",
    "spearman_df['metric'] = spearman_df.index\n",
    "spearman_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>aggressiveness_score</th>\n",
       "      <th>coherence_score</th>\n",
       "      <th>suitableness_score</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bleu_1_(pred_cs, cs)</th>\n",
       "      <td>0.032719</td>\n",
       "      <td>-0.111208</td>\n",
       "      <td>0.129568</td>\n",
       "      <td>0.059769</td>\n",
       "      <td>bleu_1_(pred_cs, cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bleu_2_(pred_cs, cs)</th>\n",
       "      <td>0.029627</td>\n",
       "      <td>0.079862</td>\n",
       "      <td>-0.021727</td>\n",
       "      <td>0.025987</td>\n",
       "      <td>bleu_2_(pred_cs, cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bleu_3_(pred_cs, cs)</th>\n",
       "      <td>0.018866</td>\n",
       "      <td>0.084688</td>\n",
       "      <td>-0.045317</td>\n",
       "      <td>0.010906</td>\n",
       "      <td>bleu_3_(pred_cs, cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bleu_4_(pred_cs, cs)</th>\n",
       "      <td>0.018866</td>\n",
       "      <td>0.084688</td>\n",
       "      <td>-0.045317</td>\n",
       "      <td>0.010906</td>\n",
       "      <td>bleu_4_(pred_cs, cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_1_(pred_cs, cs)</th>\n",
       "      <td>0.158060</td>\n",
       "      <td>-0.092367</td>\n",
       "      <td>0.314803</td>\n",
       "      <td>0.215313</td>\n",
       "      <td>rouge_1_(pred_cs, cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_2_(pred_cs, cs)</th>\n",
       "      <td>0.153730</td>\n",
       "      <td>-0.127476</td>\n",
       "      <td>0.333455</td>\n",
       "      <td>0.222577</td>\n",
       "      <td>rouge_2_(pred_cs, cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_l_(pred_cs, cs)</th>\n",
       "      <td>0.162743</td>\n",
       "      <td>-0.088069</td>\n",
       "      <td>0.319367</td>\n",
       "      <td>0.216637</td>\n",
       "      <td>rouge_l_(pred_cs, cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart_score_(cs, pred_cs)</th>\n",
       "      <td>0.165685</td>\n",
       "      <td>-0.163725</td>\n",
       "      <td>0.344131</td>\n",
       "      <td>0.219073</td>\n",
       "      <td>bart_score_(cs, pred_cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart_score_(pred_cs, cs)</th>\n",
       "      <td>0.197770</td>\n",
       "      <td>-0.110811</td>\n",
       "      <td>0.355226</td>\n",
       "      <td>0.238458</td>\n",
       "      <td>bart_score_(pred_cs, cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UniEval_aggressiveness</th>\n",
       "      <td>0.044089</td>\n",
       "      <td>0.084572</td>\n",
       "      <td>0.032911</td>\n",
       "      <td>0.067804</td>\n",
       "      <td>UniEval_aggressiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UniEval_coherence</th>\n",
       "      <td>0.030832</td>\n",
       "      <td>0.022561</td>\n",
       "      <td>0.048041</td>\n",
       "      <td>0.053460</td>\n",
       "      <td>UniEval_coherence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UniEval_relevance</th>\n",
       "      <td>0.033103</td>\n",
       "      <td>0.017570</td>\n",
       "      <td>0.051064</td>\n",
       "      <td>0.053302</td>\n",
       "      <td>UniEval_relevance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UniEval_suitableness</th>\n",
       "      <td>0.046473</td>\n",
       "      <td>0.026577</td>\n",
       "      <td>0.062299</td>\n",
       "      <td>0.066374</td>\n",
       "      <td>UniEval_suitableness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pc_score_(hs, pred_cs)</th>\n",
       "      <td>-0.031428</td>\n",
       "      <td>0.111137</td>\n",
       "      <td>-0.112981</td>\n",
       "      <td>-0.055941</td>\n",
       "      <td>pc_score_(hs, pred_cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aq_score_(pred_cs)</th>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.013249</td>\n",
       "      <td>-0.056563</td>\n",
       "      <td>-0.041903</td>\n",
       "      <td>aq_score_(pred_cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cd_score_(hs, pred_cs)</th>\n",
       "      <td>0.051775</td>\n",
       "      <td>0.026500</td>\n",
       "      <td>-0.034181</td>\n",
       "      <td>-0.003223</td>\n",
       "      <td>cd_score_(hs, pred_cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pd_score_(hs, pred_cs)</th>\n",
       "      <td>0.063280</td>\n",
       "      <td>-0.066445</td>\n",
       "      <td>0.074580</td>\n",
       "      <td>0.050846</td>\n",
       "      <td>pd_score_(hs, pred_cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative_pc_score_(hs, pred_cs)</th>\n",
       "      <td>0.031428</td>\n",
       "      <td>-0.111137</td>\n",
       "      <td>0.112981</td>\n",
       "      <td>0.055941</td>\n",
       "      <td>negative_pc_score_(hs, pred_cs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4_relevance_score</th>\n",
       "      <td>0.715072</td>\n",
       "      <td>-0.008324</td>\n",
       "      <td>0.518816</td>\n",
       "      <td>0.458615</td>\n",
       "      <td>gpt-4_relevance_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4_aggressiveness_score</th>\n",
       "      <td>-0.018181</td>\n",
       "      <td>0.622794</td>\n",
       "      <td>-0.277161</td>\n",
       "      <td>-0.081620</td>\n",
       "      <td>gpt-4_aggressiveness_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4_coherence_score</th>\n",
       "      <td>0.273484</td>\n",
       "      <td>-0.094252</td>\n",
       "      <td>0.691491</td>\n",
       "      <td>0.437177</td>\n",
       "      <td>gpt-4_coherence_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4_suitableness_score</th>\n",
       "      <td>0.254253</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.391741</td>\n",
       "      <td>0.685881</td>\n",
       "      <td>gpt-4_suitableness_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-zs_relevance_score</th>\n",
       "      <td>0.655261</td>\n",
       "      <td>-0.007710</td>\n",
       "      <td>0.474814</td>\n",
       "      <td>0.426699</td>\n",
       "      <td>gpt-4-zs_relevance_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-zs_coherence_score</th>\n",
       "      <td>0.239339</td>\n",
       "      <td>-0.086735</td>\n",
       "      <td>0.602449</td>\n",
       "      <td>0.384577</td>\n",
       "      <td>gpt-4-zs_coherence_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-zs_aggressiveness_score</th>\n",
       "      <td>-0.023473</td>\n",
       "      <td>0.531666</td>\n",
       "      <td>-0.240341</td>\n",
       "      <td>-0.070595</td>\n",
       "      <td>gpt-4-zs_aggressiveness_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-zs_suitableness_score</th>\n",
       "      <td>0.216944</td>\n",
       "      <td>-0.004589</td>\n",
       "      <td>0.337154</td>\n",
       "      <td>0.576292</td>\n",
       "      <td>gpt-4-zs_suitableness_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEVAL-4_relevance_score</th>\n",
       "      <td>0.698564</td>\n",
       "      <td>-0.004658</td>\n",
       "      <td>0.505921</td>\n",
       "      <td>0.449348</td>\n",
       "      <td>GEVAL-4_relevance_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEVAL-4_coherence_score</th>\n",
       "      <td>0.255441</td>\n",
       "      <td>-0.090663</td>\n",
       "      <td>0.639063</td>\n",
       "      <td>0.406743</td>\n",
       "      <td>GEVAL-4_coherence_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEVAL-4_aggressiveness_score</th>\n",
       "      <td>-0.020879</td>\n",
       "      <td>0.558003</td>\n",
       "      <td>-0.253643</td>\n",
       "      <td>-0.071588</td>\n",
       "      <td>GEVAL-4_aggressiveness_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEVAL-4_suitableness_score</th>\n",
       "      <td>0.243451</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>0.376321</td>\n",
       "      <td>0.658110</td>\n",
       "      <td>GEVAL-4_suitableness_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPTScore_relevance_score</th>\n",
       "      <td>0.478037</td>\n",
       "      <td>-0.002320</td>\n",
       "      <td>0.346111</td>\n",
       "      <td>0.315682</td>\n",
       "      <td>GPTScore_relevance_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPTScore_coherence_score</th>\n",
       "      <td>0.141169</td>\n",
       "      <td>-0.080695</td>\n",
       "      <td>0.398824</td>\n",
       "      <td>0.228563</td>\n",
       "      <td>GPTScore_coherence_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPTScore_aggressiveness_score</th>\n",
       "      <td>-0.011512</td>\n",
       "      <td>0.446962</td>\n",
       "      <td>-0.190108</td>\n",
       "      <td>-0.045595</td>\n",
       "      <td>GPTScore_aggressiveness_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPTScore_suitableness_score</th>\n",
       "      <td>0.155514</td>\n",
       "      <td>0.010818</td>\n",
       "      <td>0.241775</td>\n",
       "      <td>0.425440</td>\n",
       "      <td>GPTScore_suitableness_score</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 relevance_score  aggressiveness_score  \\\n",
       "bleu_1_(pred_cs, cs)                    0.032719             -0.111208   \n",
       "bleu_2_(pred_cs, cs)                    0.029627              0.079862   \n",
       "bleu_3_(pred_cs, cs)                    0.018866              0.084688   \n",
       "bleu_4_(pred_cs, cs)                    0.018866              0.084688   \n",
       "rouge_1_(pred_cs, cs)                   0.158060             -0.092367   \n",
       "rouge_2_(pred_cs, cs)                   0.153730             -0.127476   \n",
       "rouge_l_(pred_cs, cs)                   0.162743             -0.088069   \n",
       "bart_score_(cs, pred_cs)                0.165685             -0.163725   \n",
       "bart_score_(pred_cs, cs)                0.197770             -0.110811   \n",
       "UniEval_aggressiveness                  0.044089              0.084572   \n",
       "UniEval_coherence                       0.030832              0.022561   \n",
       "UniEval_relevance                       0.033103              0.017570   \n",
       "UniEval_suitableness                    0.046473              0.026577   \n",
       "pc_score_(hs, pred_cs)                 -0.031428              0.111137   \n",
       "aq_score_(pred_cs)                      0.016992              0.013249   \n",
       "cd_score_(hs, pred_cs)                  0.051775              0.026500   \n",
       "pd_score_(hs, pred_cs)                  0.063280             -0.066445   \n",
       "negative_pc_score_(hs, pred_cs)         0.031428             -0.111137   \n",
       "gpt-4_relevance_score                   0.715072             -0.008324   \n",
       "gpt-4_aggressiveness_score             -0.018181              0.622794   \n",
       "gpt-4_coherence_score                   0.273484             -0.094252   \n",
       "gpt-4_suitableness_score                0.254253              0.003146   \n",
       "gpt-4-zs_relevance_score                0.655261             -0.007710   \n",
       "gpt-4-zs_coherence_score                0.239339             -0.086735   \n",
       "gpt-4-zs_aggressiveness_score          -0.023473              0.531666   \n",
       "gpt-4-zs_suitableness_score             0.216944             -0.004589   \n",
       "GEVAL-4_relevance_score                 0.698564             -0.004658   \n",
       "GEVAL-4_coherence_score                 0.255441             -0.090663   \n",
       "GEVAL-4_aggressiveness_score           -0.020879              0.558003   \n",
       "GEVAL-4_suitableness_score              0.243451             -0.000082   \n",
       "GPTScore_relevance_score                0.478037             -0.002320   \n",
       "GPTScore_coherence_score                0.141169             -0.080695   \n",
       "GPTScore_aggressiveness_score          -0.011512              0.446962   \n",
       "GPTScore_suitableness_score             0.155514              0.010818   \n",
       "\n",
       "                                 coherence_score  suitableness_score  \\\n",
       "bleu_1_(pred_cs, cs)                    0.129568            0.059769   \n",
       "bleu_2_(pred_cs, cs)                   -0.021727            0.025987   \n",
       "bleu_3_(pred_cs, cs)                   -0.045317            0.010906   \n",
       "bleu_4_(pred_cs, cs)                   -0.045317            0.010906   \n",
       "rouge_1_(pred_cs, cs)                   0.314803            0.215313   \n",
       "rouge_2_(pred_cs, cs)                   0.333455            0.222577   \n",
       "rouge_l_(pred_cs, cs)                   0.319367            0.216637   \n",
       "bart_score_(cs, pred_cs)                0.344131            0.219073   \n",
       "bart_score_(pred_cs, cs)                0.355226            0.238458   \n",
       "UniEval_aggressiveness                  0.032911            0.067804   \n",
       "UniEval_coherence                       0.048041            0.053460   \n",
       "UniEval_relevance                       0.051064            0.053302   \n",
       "UniEval_suitableness                    0.062299            0.066374   \n",
       "pc_score_(hs, pred_cs)                 -0.112981           -0.055941   \n",
       "aq_score_(pred_cs)                     -0.056563           -0.041903   \n",
       "cd_score_(hs, pred_cs)                 -0.034181           -0.003223   \n",
       "pd_score_(hs, pred_cs)                  0.074580            0.050846   \n",
       "negative_pc_score_(hs, pred_cs)         0.112981            0.055941   \n",
       "gpt-4_relevance_score                   0.518816            0.458615   \n",
       "gpt-4_aggressiveness_score             -0.277161           -0.081620   \n",
       "gpt-4_coherence_score                   0.691491            0.437177   \n",
       "gpt-4_suitableness_score                0.391741            0.685881   \n",
       "gpt-4-zs_relevance_score                0.474814            0.426699   \n",
       "gpt-4-zs_coherence_score                0.602449            0.384577   \n",
       "gpt-4-zs_aggressiveness_score          -0.240341           -0.070595   \n",
       "gpt-4-zs_suitableness_score             0.337154            0.576292   \n",
       "GEVAL-4_relevance_score                 0.505921            0.449348   \n",
       "GEVAL-4_coherence_score                 0.639063            0.406743   \n",
       "GEVAL-4_aggressiveness_score           -0.253643           -0.071588   \n",
       "GEVAL-4_suitableness_score              0.376321            0.658110   \n",
       "GPTScore_relevance_score                0.346111            0.315682   \n",
       "GPTScore_coherence_score                0.398824            0.228563   \n",
       "GPTScore_aggressiveness_score          -0.190108           -0.045595   \n",
       "GPTScore_suitableness_score             0.241775            0.425440   \n",
       "\n",
       "                                                          metric  \n",
       "bleu_1_(pred_cs, cs)                        bleu_1_(pred_cs, cs)  \n",
       "bleu_2_(pred_cs, cs)                        bleu_2_(pred_cs, cs)  \n",
       "bleu_3_(pred_cs, cs)                        bleu_3_(pred_cs, cs)  \n",
       "bleu_4_(pred_cs, cs)                        bleu_4_(pred_cs, cs)  \n",
       "rouge_1_(pred_cs, cs)                      rouge_1_(pred_cs, cs)  \n",
       "rouge_2_(pred_cs, cs)                      rouge_2_(pred_cs, cs)  \n",
       "rouge_l_(pred_cs, cs)                      rouge_l_(pred_cs, cs)  \n",
       "bart_score_(cs, pred_cs)                bart_score_(cs, pred_cs)  \n",
       "bart_score_(pred_cs, cs)                bart_score_(pred_cs, cs)  \n",
       "UniEval_aggressiveness                    UniEval_aggressiveness  \n",
       "UniEval_coherence                              UniEval_coherence  \n",
       "UniEval_relevance                              UniEval_relevance  \n",
       "UniEval_suitableness                        UniEval_suitableness  \n",
       "pc_score_(hs, pred_cs)                    pc_score_(hs, pred_cs)  \n",
       "aq_score_(pred_cs)                            aq_score_(pred_cs)  \n",
       "cd_score_(hs, pred_cs)                    cd_score_(hs, pred_cs)  \n",
       "pd_score_(hs, pred_cs)                    pd_score_(hs, pred_cs)  \n",
       "negative_pc_score_(hs, pred_cs)  negative_pc_score_(hs, pred_cs)  \n",
       "gpt-4_relevance_score                      gpt-4_relevance_score  \n",
       "gpt-4_aggressiveness_score            gpt-4_aggressiveness_score  \n",
       "gpt-4_coherence_score                      gpt-4_coherence_score  \n",
       "gpt-4_suitableness_score                gpt-4_suitableness_score  \n",
       "gpt-4-zs_relevance_score                gpt-4-zs_relevance_score  \n",
       "gpt-4-zs_coherence_score                gpt-4-zs_coherence_score  \n",
       "gpt-4-zs_aggressiveness_score      gpt-4-zs_aggressiveness_score  \n",
       "gpt-4-zs_suitableness_score          gpt-4-zs_suitableness_score  \n",
       "GEVAL-4_relevance_score                  GEVAL-4_relevance_score  \n",
       "GEVAL-4_coherence_score                  GEVAL-4_coherence_score  \n",
       "GEVAL-4_aggressiveness_score        GEVAL-4_aggressiveness_score  \n",
       "GEVAL-4_suitableness_score            GEVAL-4_suitableness_score  \n",
       "GPTScore_relevance_score                GPTScore_relevance_score  \n",
       "GPTScore_coherence_score                GPTScore_coherence_score  \n",
       "GPTScore_aggressiveness_score      GPTScore_aggressiveness_score  \n",
       "GPTScore_suitableness_score          GPTScore_suitableness_score  "
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kendall_df['metric'] = kendall_df.index\n",
    "kendall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>relevance_score_spearman</th>\n",
       "      <th>aggressiveness_score_spearman</th>\n",
       "      <th>coherence_score_spearman</th>\n",
       "      <th>suitableness_score_spearman</th>\n",
       "      <th>relevance_score_kendall</th>\n",
       "      <th>aggressiveness_score_kendall</th>\n",
       "      <th>coherence_score_kendall</th>\n",
       "      <th>suitableness_score_kendall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UniEval_aggressiveness</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.111082</td>\n",
       "      <td>0.044383</td>\n",
       "      <td>0.085432</td>\n",
       "      <td>0.044089</td>\n",
       "      <td>0.084572</td>\n",
       "      <td>0.032911</td>\n",
       "      <td>0.067804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UniEval_coherence</td>\n",
       "      <td>0.040108</td>\n",
       "      <td>0.030535</td>\n",
       "      <td>0.065817</td>\n",
       "      <td>0.067192</td>\n",
       "      <td>0.030832</td>\n",
       "      <td>0.022561</td>\n",
       "      <td>0.048041</td>\n",
       "      <td>0.053460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UniEval_relevance</td>\n",
       "      <td>0.043203</td>\n",
       "      <td>0.024161</td>\n",
       "      <td>0.069883</td>\n",
       "      <td>0.067020</td>\n",
       "      <td>0.033103</td>\n",
       "      <td>0.017570</td>\n",
       "      <td>0.051064</td>\n",
       "      <td>0.053302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UniEval_suitableness</td>\n",
       "      <td>0.060316</td>\n",
       "      <td>0.035836</td>\n",
       "      <td>0.085194</td>\n",
       "      <td>0.083354</td>\n",
       "      <td>0.046473</td>\n",
       "      <td>0.026577</td>\n",
       "      <td>0.062299</td>\n",
       "      <td>0.066374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pc_score_(hs, pred_cs)</td>\n",
       "      <td>-0.040647</td>\n",
       "      <td>0.145030</td>\n",
       "      <td>-0.155956</td>\n",
       "      <td>-0.070306</td>\n",
       "      <td>-0.031428</td>\n",
       "      <td>0.111137</td>\n",
       "      <td>-0.112981</td>\n",
       "      <td>-0.055941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aq_score_(pred_cs)</td>\n",
       "      <td>0.022270</td>\n",
       "      <td>0.017832</td>\n",
       "      <td>-0.076050</td>\n",
       "      <td>-0.052492</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.013249</td>\n",
       "      <td>-0.056563</td>\n",
       "      <td>-0.041903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cd_score_(hs, pred_cs)</td>\n",
       "      <td>0.067713</td>\n",
       "      <td>0.034492</td>\n",
       "      <td>-0.046179</td>\n",
       "      <td>-0.003674</td>\n",
       "      <td>0.051775</td>\n",
       "      <td>0.026500</td>\n",
       "      <td>-0.034181</td>\n",
       "      <td>-0.003223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pd_score_(hs, pred_cs)</td>\n",
       "      <td>0.081940</td>\n",
       "      <td>-0.086723</td>\n",
       "      <td>0.102084</td>\n",
       "      <td>0.063634</td>\n",
       "      <td>0.063280</td>\n",
       "      <td>-0.066445</td>\n",
       "      <td>0.074580</td>\n",
       "      <td>0.050846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>negative_pc_score_(hs, pred_cs)</td>\n",
       "      <td>0.040647</td>\n",
       "      <td>-0.145030</td>\n",
       "      <td>0.155956</td>\n",
       "      <td>0.070306</td>\n",
       "      <td>0.031428</td>\n",
       "      <td>-0.111137</td>\n",
       "      <td>0.112981</td>\n",
       "      <td>0.055941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            metric  relevance_score_spearman  \\\n",
       "0           UniEval_aggressiveness                  0.057200   \n",
       "1                UniEval_coherence                  0.040108   \n",
       "2                UniEval_relevance                  0.043203   \n",
       "3             UniEval_suitableness                  0.060316   \n",
       "4           pc_score_(hs, pred_cs)                 -0.040647   \n",
       "5               aq_score_(pred_cs)                  0.022270   \n",
       "6           cd_score_(hs, pred_cs)                  0.067713   \n",
       "7           pd_score_(hs, pred_cs)                  0.081940   \n",
       "8  negative_pc_score_(hs, pred_cs)                  0.040647   \n",
       "\n",
       "   aggressiveness_score_spearman  coherence_score_spearman  \\\n",
       "0                       0.111082                  0.044383   \n",
       "1                       0.030535                  0.065817   \n",
       "2                       0.024161                  0.069883   \n",
       "3                       0.035836                  0.085194   \n",
       "4                       0.145030                 -0.155956   \n",
       "5                       0.017832                 -0.076050   \n",
       "6                       0.034492                 -0.046179   \n",
       "7                      -0.086723                  0.102084   \n",
       "8                      -0.145030                  0.155956   \n",
       "\n",
       "   suitableness_score_spearman  relevance_score_kendall  \\\n",
       "0                     0.085432                 0.044089   \n",
       "1                     0.067192                 0.030832   \n",
       "2                     0.067020                 0.033103   \n",
       "3                     0.083354                 0.046473   \n",
       "4                    -0.070306                -0.031428   \n",
       "5                    -0.052492                 0.016992   \n",
       "6                    -0.003674                 0.051775   \n",
       "7                     0.063634                 0.063280   \n",
       "8                     0.070306                 0.031428   \n",
       "\n",
       "   aggressiveness_score_kendall  coherence_score_kendall  \\\n",
       "0                      0.084572                 0.032911   \n",
       "1                      0.022561                 0.048041   \n",
       "2                      0.017570                 0.051064   \n",
       "3                      0.026577                 0.062299   \n",
       "4                      0.111137                -0.112981   \n",
       "5                      0.013249                -0.056563   \n",
       "6                      0.026500                -0.034181   \n",
       "7                     -0.066445                 0.074580   \n",
       "8                     -0.111137                 0.112981   \n",
       "\n",
       "   suitableness_score_kendall  \n",
       "0                    0.067804  \n",
       "1                    0.053460  \n",
       "2                    0.053302  \n",
       "3                    0.066374  \n",
       "4                   -0.055941  \n",
       "5                   -0.041903  \n",
       "6                   -0.003223  \n",
       "7                    0.050846  \n",
       "8                    0.055941  "
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_results = pd.merge(spearman_df, kendall_df, on=['metric'], suffixes=['_spearman', '_kendall'])\n",
    "# Remove the column you want to move\n",
    "first_column = merged_results.pop('metric')\n",
    "# Insert the column at the first position\n",
    "merged_results.insert(0, 'metric', first_column)\n",
    "merged_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "gpt-4_relevance_score\n",
      "(12447, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1758196/1597700783.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_temp.rename(columns={col: col.replace('gpt-4_','calibrated-cot-gpt4')}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4_aggressiveness_score\n",
      "(12447, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1758196/1597700783.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_temp.rename(columns={col: col.replace('gpt-4_','calibrated-cot-gpt4')}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4_coherence_score\n",
      "(12447, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1758196/1597700783.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_temp.rename(columns={col: col.replace('gpt-4_','calibrated-cot-gpt4')}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4_suitableness_score\n",
      "(12447, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1758196/1597700783.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_temp.rename(columns={col: col.replace('gpt-4_','calibrated-cot-gpt4')}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n",
      "(12447, 8)\n"
     ]
    }
   ],
   "source": [
    "for col in automated_metrics:\n",
    "    df_temp = df[['hatespeech','counterspeech','predicted_counterspeech', 'relevance_score','coherence_score','aggressiveness_score','suitableness_score',f'{col}']]\n",
    "    if 'gpt-4_' in col:\n",
    "        print(col)\n",
    "        df_temp.rename(columns={col: col.replace('gpt-4_','calibrated-cot-gpt4')}, inplace=True)\n",
    "        col = col.replace('gpt-4_','calibrated-cot-gpt4')\n",
    "    print(df_temp.shape)\n",
    "    df_temp.to_csv(f'/home/ameyh/counterspeech-EVAL/CSEval/runs/pred_metric_{col}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results.to_csv('/home/ameyh/counterspeech-EVAL/CSEval/results_aggregated_cleaned_round_upformat-3.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/ameyh/cs-eval/final_data/annotations_final_for_gods_sake.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2296"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['counterspeech'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is the dataframe with the four columns.\n",
    "# Let's create a sample dataframe with dummy data.\n",
    "# In your case, you should replace this with:\n",
    "# df = pd.read_csv('path_to_your_csv.csv') or any other method to load your dataframe.\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Coherence': np.random.normal(loc=0, scale=1, size=500),\n",
    "    'Consistency': np.random.normal(loc=0, scale=1, size=500),\n",
    "    'Fluency': np.random.normal(loc=0, scale=1, size=500),\n",
    "    'Relevance': np.random.normal(loc=0, scale=1, size=500)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 5))\n",
    "\n",
    "# Define bin edges for histograms\n",
    "bin_edges = [-2, -1, 0, 1, 2]\n",
    "\n",
    "# Titles for subplots\n",
    "titles = ['Coherence', 'Consistency', 'Fluency', 'Relevance']\n",
    "\n",
    "for ax, column, title in zip(axes, df.columns, titles):\n",
    "    ax.hist(df[column], bins=bin_edges, color='blue', edgecolor='black')\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylim(0, 500)  # Adjust y-axis limits to match the example image\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Main title\n",
    "plt.suptitle('Standard Deviation Bins\\nExpert Annotations Round 1', y=1.10, fontsize=14)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hatespeech', 'counterspeech', 'predicted_counterspeech', 'csType',\n",
      "       'source', 'prediction_(prompt_aggressiveness_score)_(gpt-4)',\n",
      "       'prediction_(prompt_relevance_score)_(gpt-4)', 'uuid',\n",
      "       'prediction_(prompt_aggressiveness_score)_(gpt3.5-turbo)',\n",
      "       'prediction_(prompt_coherence_score)_(gpt-3.5-turbo)',\n",
      "       'prediction_(prompt_relevance_score)_(gpt-3.5-turbo)',\n",
      "       'prediction_(prompt_suitableness_score)_(gpt-3.5-turbo)',\n",
      "       'aggressiveness_score', 'coherence_score', 'relevance_score',\n",
      "       'suitableness_score', 'obscenity_(pred_cs)',\n",
      "       'identity_attack_(pred_cs)', 'insult_(pred_cs)', 'bleu_1_(cs, pred_cs)',\n",
      "       'bleu_2_(cs, pred_cs)', 'cosine_similarity_(hs, pred_cs)',\n",
      "       'rouge_l_(cs, pred_cs)', 'rouge_1_(cs, pred_cs)',\n",
      "       'rouge_2_(cs, pred_cs)', 'meteor_score_(cs, pred_cs)',\n",
      "       'bert_score_(hs, pred_cs)', 'toxicity_(pred_cs)',\n",
      "       'pc_score_(hs, pred_cs)', 'cd_score_(hs, pred_cs)',\n",
      "       'aq_score_(pred_cs)', 'bm25_score_(hs, pred_cs)', 'mauve_score',\n",
      "       'bart_score_(hs,pred_cs)', 'bart_score_(cs,pred_cs)',\n",
      "       'coherence_UniEval_(hs, cs, pred_cs)',\n",
      "       'consistency_UniEval_(hs, cs, pred_cs)',\n",
      "       'fluency_UniEval_(hs, cs, pred_cs)',\n",
      "       'relevance_UniEval_(hs, cs, pred_cs)',\n",
      "       'aggressiveness_score_annotator_1', 'aggressiveness_score_annotator_2',\n",
      "       'aggressiveness_score_annotator_3', 'aggressiveness_score_annotator_4',\n",
      "       'aggressiveness_score_annotator_5', 'coherence_score_annotator_1',\n",
      "       'coherence_score_annotator_2', 'coherence_score_annotator_3',\n",
      "       'coherence_score_annotator_4', 'coherence_score_annotator_5',\n",
      "       'relevance_score_annotator_1', 'relevance_score_annotator_2',\n",
      "       'relevance_score_annotator_3', 'relevance_score_annotator_4',\n",
      "       'relevance_score_annotator_5', 'suitableness_score_annotator_1',\n",
      "       'suitableness_score_annotator_2', 'suitableness_score_annotator_3',\n",
      "       'suitableness_score_annotator_4', 'suitableness_score_annotator_5'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>counterspeech</th>\n",
       "      <th>predicted_counterspeech</th>\n",
       "      <th>csType</th>\n",
       "      <th>source</th>\n",
       "      <th>prediction_(prompt_aggressiveness_score)_(gpt-4)</th>\n",
       "      <th>prediction_(prompt_relevance_score)_(gpt-4)</th>\n",
       "      <th>uuid</th>\n",
       "      <th>prediction_(prompt_aggressiveness_score)_(gpt3.5-turbo)</th>\n",
       "      <th>prediction_(prompt_coherence_score)_(gpt-3.5-turbo)</th>\n",
       "      <th>...</th>\n",
       "      <th>relevance_score_annotator_1</th>\n",
       "      <th>relevance_score_annotator_2</th>\n",
       "      <th>relevance_score_annotator_3</th>\n",
       "      <th>relevance_score_annotator_4</th>\n",
       "      <th>relevance_score_annotator_5</th>\n",
       "      <th>suitableness_score_annotator_1</th>\n",
       "      <th>suitableness_score_annotator_2</th>\n",
       "      <th>suitableness_score_annotator_3</th>\n",
       "      <th>suitableness_score_annotator_4</th>\n",
       "      <th>suitableness_score_annotator_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Every Muslim is a potential terrorist, unfit t...</td>\n",
       "      <td>I understand if you have concerns about Muslim...</td>\n",
       "      <td>It is crucial to deal with this issue with fee...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>GPT3.5_ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e3eb1ff9-eee0-4d81-b953-02ee79e7364f</td>\n",
       "      <td>{'Aggressiveness': '1'}</td>\n",
       "      <td>{'Argument Coherence': '5'}</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          hatespeech  \\\n",
       "0  Every Muslim is a potential terrorist, unfit t...   \n",
       "\n",
       "                                       counterspeech  \\\n",
       "0  I understand if you have concerns about Muslim...   \n",
       "\n",
       "                             predicted_counterspeech    csType     source  \\\n",
       "0  It is crucial to deal with this issue with fee...  Positive  GPT3.5_ZS   \n",
       "\n",
       "  prediction_(prompt_aggressiveness_score)_(gpt-4)  \\\n",
       "0                                              NaN   \n",
       "\n",
       "  prediction_(prompt_relevance_score)_(gpt-4)  \\\n",
       "0                                         NaN   \n",
       "\n",
       "                                   uuid  \\\n",
       "0  e3eb1ff9-eee0-4d81-b953-02ee79e7364f   \n",
       "\n",
       "  prediction_(prompt_aggressiveness_score)_(gpt3.5-turbo)  \\\n",
       "0                            {'Aggressiveness': '1'}        \n",
       "\n",
       "  prediction_(prompt_coherence_score)_(gpt-3.5-turbo)  ...  \\\n",
       "0                        {'Argument Coherence': '5'}   ...   \n",
       "\n",
       "  relevance_score_annotator_1 relevance_score_annotator_2  \\\n",
       "0                           5                           4   \n",
       "\n",
       "   relevance_score_annotator_3  relevance_score_annotator_4  \\\n",
       "0                            5                            5   \n",
       "\n",
       "   relevance_score_annotator_5  suitableness_score_annotator_1  \\\n",
       "0                            4                               4   \n",
       "\n",
       "   suitableness_score_annotator_2  suitableness_score_annotator_3  \\\n",
       "0                               2                               2   \n",
       "\n",
       "   suitableness_score_annotator_4  suitableness_score_annotator_5  \n",
       "0                               3                               4  \n",
       "\n",
       "[1 rows x 59 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_500 = pd.read_csv('/home/ameyh/cs-eval/annotations/annotations_first_pass_500.csv')\n",
    "print(df_500.columns)\n",
    "df_500.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_500['relevance_score_std_dev'] = df_500[['relevance_score_annotator_1','relevance_score_annotator_2','relevance_score_annotator_3','relevance_score_annotator_4','relevance_score_annotator_5']].std(axis=1)\n",
    "df_500['aggressiveness_score_std_dev'] = df_500[['aggressiveness_score_annotator_1','aggressiveness_score_annotator_2','aggressiveness_score_annotator_3','aggressiveness_score_annotator_4','aggressiveness_score_annotator_5']].std(axis=1)\n",
    "df_500['coherence_score_std_dev'] = df_500[['coherence_score_annotator_1','coherence_score_annotator_2','coherence_score_annotator_3','coherence_score_annotator_4','coherence_score_annotator_5']].std(axis=1)\n",
    "df_500['suitableness_score_std_dev'] = df_500[['suitableness_score_annotator_1','suitableness_score_annotator_2','suitableness_score_annotator_3','suitableness_score_annotator_4','suitableness_score_annotator_5']].std(axis=1)\n",
    "\n",
    "# df_500['coherence_score_std_dev'].hist(bins=8)\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 5))\n",
    "\n",
    "# Define bin edges for histograms\n",
    "bin_edges = [0,0.25,0.5,1,1.25,1.5]\n",
    "\n",
    "# Titles for subplots\n",
    "titles = ['Contextual Relevance', 'Aggressiveness', 'Argument Coherence', 'Suitableness']\n",
    "use_cols = ['relevance_score_std_dev', 'aggressiveness_score_std_dev', 'coherence_score_std_dev', 'suitableness_score_std_dev']\n",
    "\n",
    "for ax, column, title in zip(axes, df_500[use_cols].columns, titles):\n",
    "    ax.hist(df_500[column], bins=bin_edges, color='#2b99ca', edgecolor='black')\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylim(0, 300)  # Adjust y-axis limits to match the example image\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Main title\n",
    "plt.suptitle('Standard Deviation Bins\\nExpert Annotations Round - 1', y=1.10, fontsize=14)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hatespeech', 'counterspeech', 'predicted_counterspeech', 'csType',\n",
      "       'source', 'prediction_(prompt_aggressiveness_score)_(gpt-4)',\n",
      "       'prediction_(prompt_relevance_score)_(gpt-4)', 'uuid',\n",
      "       'prediction_(prompt_aggressiveness_score)_(gpt3.5-turbo)',\n",
      "       'prediction_(prompt_coherence_score)_(gpt-3.5-turbo)',\n",
      "       'prediction_(prompt_relevance_score)_(gpt-3.5-turbo)',\n",
      "       'prediction_(prompt_suitableness_score)_(gpt-3.5-turbo)',\n",
      "       'aggressiveness_score', 'coherence_score', 'relevance_score',\n",
      "       'suitableness_score', 'obscenity_(pred_cs)',\n",
      "       'identity_attack_(pred_cs)', 'insult_(pred_cs)', 'bleu_1_(cs, pred_cs)',\n",
      "       'bleu_2_(cs, pred_cs)', 'cosine_similarity_(hs, pred_cs)',\n",
      "       'rouge_l_(cs, pred_cs)', 'rouge_1_(cs, pred_cs)',\n",
      "       'rouge_2_(cs, pred_cs)', 'meteor_score_(cs, pred_cs)',\n",
      "       'bert_score_(hs, pred_cs)', 'toxicity_(pred_cs)',\n",
      "       'pc_score_(hs, pred_cs)', 'cd_score_(hs, pred_cs)',\n",
      "       'aq_score_(pred_cs)', 'bm25_score_(hs, pred_cs)', 'mauve_score',\n",
      "       'bart_score_(hs,pred_cs)', 'bart_score_(cs,pred_cs)',\n",
      "       'coherence_UniEval_(hs, cs, pred_cs)',\n",
      "       'consistency_UniEval_(hs, cs, pred_cs)',\n",
      "       'fluency_UniEval_(hs, cs, pred_cs)',\n",
      "       'relevance_UniEval_(hs, cs, pred_cs)', 'suitableness_score_annotator_1',\n",
      "       'suitableness_score_annotator_2', 'suitableness_score_annotator_3',\n",
      "       'suitableness_score_annotator_4', 'suitableness_score_annotator_5',\n",
      "       'relevance_score_annotator_1', 'relevance_score_annotator_2',\n",
      "       'relevance_score_annotator_3', 'relevance_score_annotator_4',\n",
      "       'relevance_score_annotator_5', 'aggressiveness_score_annotator_1',\n",
      "       'aggressiveness_score_annotator_2', 'aggressiveness_score_annotator_3',\n",
      "       'aggressiveness_score_annotator_4', 'aggressiveness_score_annotator_5',\n",
      "       'coherence_score_annotator_1', 'coherence_score_annotator_2',\n",
      "       'coherence_score_annotator_3', 'coherence_score_annotator_4',\n",
      "       'coherence_score_annotator_5'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>counterspeech</th>\n",
       "      <th>predicted_counterspeech</th>\n",
       "      <th>csType</th>\n",
       "      <th>source</th>\n",
       "      <th>prediction_(prompt_aggressiveness_score)_(gpt-4)</th>\n",
       "      <th>prediction_(prompt_relevance_score)_(gpt-4)</th>\n",
       "      <th>uuid</th>\n",
       "      <th>prediction_(prompt_aggressiveness_score)_(gpt3.5-turbo)</th>\n",
       "      <th>prediction_(prompt_coherence_score)_(gpt-3.5-turbo)</th>\n",
       "      <th>...</th>\n",
       "      <th>aggressiveness_score_annotator_1</th>\n",
       "      <th>aggressiveness_score_annotator_2</th>\n",
       "      <th>aggressiveness_score_annotator_3</th>\n",
       "      <th>aggressiveness_score_annotator_4</th>\n",
       "      <th>aggressiveness_score_annotator_5</th>\n",
       "      <th>coherence_score_annotator_1</th>\n",
       "      <th>coherence_score_annotator_2</th>\n",
       "      <th>coherence_score_annotator_3</th>\n",
       "      <th>coherence_score_annotator_4</th>\n",
       "      <th>coherence_score_annotator_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muslims have no place in a modern democracy li...</td>\n",
       "      <td>Isn't divesity essential to a modern democracy?</td>\n",
       "      <td>I understand that you have a perspective on Mu...</td>\n",
       "      <td>Questioning</td>\n",
       "      <td>GPT3.5_ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>165a7d8b-c325-4631-91b7-2bf5082a1054</td>\n",
       "      <td>{'Aggressiveness': '1'}</td>\n",
       "      <td>{'Argument Coherence': '1'}</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          hatespeech  \\\n",
       "0  Muslims have no place in a modern democracy li...   \n",
       "\n",
       "                                     counterspeech  \\\n",
       "0  Isn't divesity essential to a modern democracy?   \n",
       "\n",
       "                             predicted_counterspeech       csType     source  \\\n",
       "0  I understand that you have a perspective on Mu...  Questioning  GPT3.5_ZS   \n",
       "\n",
       "  prediction_(prompt_aggressiveness_score)_(gpt-4)  \\\n",
       "0                                              NaN   \n",
       "\n",
       "  prediction_(prompt_relevance_score)_(gpt-4)  \\\n",
       "0                                         NaN   \n",
       "\n",
       "                                   uuid  \\\n",
       "0  165a7d8b-c325-4631-91b7-2bf5082a1054   \n",
       "\n",
       "  prediction_(prompt_aggressiveness_score)_(gpt3.5-turbo)  \\\n",
       "0                            {'Aggressiveness': '1'}        \n",
       "\n",
       "  prediction_(prompt_coherence_score)_(gpt-3.5-turbo)  ...  \\\n",
       "0                        {'Argument Coherence': '1'}   ...   \n",
       "\n",
       "  aggressiveness_score_annotator_1 aggressiveness_score_annotator_2  \\\n",
       "0                                1                                1   \n",
       "\n",
       "   aggressiveness_score_annotator_3  aggressiveness_score_annotator_4  \\\n",
       "0                                 1                                 1   \n",
       "\n",
       "   aggressiveness_score_annotator_5  coherence_score_annotator_1  \\\n",
       "0                                 1                            2   \n",
       "\n",
       "   coherence_score_annotator_2  coherence_score_annotator_3  \\\n",
       "0                            2                            1   \n",
       "\n",
       "   coherence_score_annotator_4  coherence_score_annotator_5  \n",
       "0                            2                            1  \n",
       "\n",
       "[1 rows x 59 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1500 = pd.read_csv('/home/ameyh/cs-eval/annotations/annotations_second_pass_1500_.csv')\n",
    "print(df_1500.columns)\n",
    "df_1500.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_1500['relevance_score_std_dev'] = df_1500[['relevance_score_annotator_1','relevance_score_annotator_2','relevance_score_annotator_3','relevance_score_annotator_4','relevance_score_annotator_5']].std(axis=1)\n",
    "df_1500['aggressiveness_score_std_dev'] = df_1500[['aggressiveness_score_annotator_1','aggressiveness_score_annotator_2','aggressiveness_score_annotator_3','aggressiveness_score_annotator_4','aggressiveness_score_annotator_5']].std(axis=1)\n",
    "df_1500['coherence_score_std_dev'] = df_1500[['coherence_score_annotator_1','coherence_score_annotator_2','coherence_score_annotator_3','coherence_score_annotator_4','coherence_score_annotator_5']].std(axis=1)\n",
    "df_1500['suitableness_score_std_dev'] = df_1500[['suitableness_score_annotator_1','suitableness_score_annotator_2','suitableness_score_annotator_3','suitableness_score_annotator_4','suitableness_score_annotator_5']].std(axis=1)\n",
    "\n",
    "# df_500['coherence_score_std_dev'].hist(bins=8)\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 5))\n",
    "\n",
    "# Define bin edges for histograms\n",
    "bin_edges = [0,0.25,0.5,1,1.25,1.5]\n",
    "\n",
    "# Titles for subplots\n",
    "titles = ['Contextual Relevance', 'Aggressiveness', 'Argument Coherence', 'Suitableness']\n",
    "use_cols = ['relevance_score_std_dev', 'aggressiveness_score_std_dev', 'coherence_score_std_dev', 'suitableness_score_std_dev']\n",
    "\n",
    "for ax, column, title in zip(axes, df_1500[use_cols].columns, titles):\n",
    "    ax.hist(df_1500[column], bins=bin_edges, color='#2b99ca', edgecolor='black')\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylim(0, 1500)  # Adjust y-axis limits to match the example image\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Main title\n",
    "plt.suptitle('Standard Deviation Bins\\nExpert Annotations Round - 2', y=1.10, fontsize=14)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>counterspeech</th>\n",
       "      <th>predicted_counterspeech</th>\n",
       "      <th>csType</th>\n",
       "      <th>source</th>\n",
       "      <th>prediction_(prompt_aggressiveness_score)_(gpt-4)</th>\n",
       "      <th>prediction_(prompt_relevance_score)_(gpt-4)</th>\n",
       "      <th>uuid</th>\n",
       "      <th>prediction_(prompt_aggressiveness_score)_(gpt3.5-turbo)</th>\n",
       "      <th>prediction_(prompt_coherence_score)_(gpt-3.5-turbo)</th>\n",
       "      <th>...</th>\n",
       "      <th>rouge_l_(pred_cs, cs)</th>\n",
       "      <th>rouge_1_(pred_cs, cs)</th>\n",
       "      <th>rouge_2_(pred_cs, cs)</th>\n",
       "      <th>meteor_score_(pred_cs, cs)</th>\n",
       "      <th>bert_score_(hs, pred_cs)</th>\n",
       "      <th>toxicity_(pred_cs)</th>\n",
       "      <th>pc_score_(hs, pred_cs)</th>\n",
       "      <th>cd_score_(hs, pred_cs)</th>\n",
       "      <th>aq_score_(pred_cs)</th>\n",
       "      <th>bm25_score_(hs, pred_cs)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Women are basically childlike, they remain thi...</td>\n",
       "      <td>The characteristics you are referring to are a...</td>\n",
       "      <td>There are many cases of women being raped by m...</td>\n",
       "      <td>Informative</td>\n",
       "      <td>dialoGPT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{ \\n'Relevance': '1'\\n}</td>\n",
       "      <td>511dd967-f04a-4bc4-a557-800af1656a96</td>\n",
       "      <td>{'Aggressiveness': '2'}</td>\n",
       "      <td>{ \\n'Coherence': '3'\\n}</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.25641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129151</td>\n",
       "      <td>0.862441</td>\n",
       "      <td>0.283898</td>\n",
       "      <td>0.981284</td>\n",
       "      <td>0.698875</td>\n",
       "      <td>0.813657</td>\n",
       "      <td>-0.545166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          hatespeech  \\\n",
       "0  Women are basically childlike, they remain thi...   \n",
       "\n",
       "                                       counterspeech  \\\n",
       "0  The characteristics you are referring to are a...   \n",
       "\n",
       "                             predicted_counterspeech       csType    source  \\\n",
       "0  There are many cases of women being raped by m...  Informative  dialoGPT   \n",
       "\n",
       "  prediction_(prompt_aggressiveness_score)_(gpt-4)  \\\n",
       "0                                              NaN   \n",
       "\n",
       "  prediction_(prompt_relevance_score)_(gpt-4)  \\\n",
       "0                     { \\n'Relevance': '1'\\n}   \n",
       "\n",
       "                                   uuid  \\\n",
       "0  511dd967-f04a-4bc4-a557-800af1656a96   \n",
       "\n",
       "  prediction_(prompt_aggressiveness_score)_(gpt3.5-turbo)  \\\n",
       "0                            {'Aggressiveness': '2'}        \n",
       "\n",
       "  prediction_(prompt_coherence_score)_(gpt-3.5-turbo)  ...  \\\n",
       "0                            { \\n'Coherence': '3'\\n}   ...   \n",
       "\n",
       "  rouge_l_(pred_cs, cs) rouge_1_(pred_cs, cs)  rouge_2_(pred_cs, cs)  \\\n",
       "0              0.205128               0.25641                    0.0   \n",
       "\n",
       "   meteor_score_(pred_cs, cs)  bert_score_(hs, pred_cs)  toxicity_(pred_cs)  \\\n",
       "0                    0.129151                  0.862441            0.283898   \n",
       "\n",
       "   pc_score_(hs, pred_cs)  cd_score_(hs, pred_cs)  aq_score_(pred_cs)  \\\n",
       "0                0.981284                0.698875            0.813657   \n",
       "\n",
       "   bm25_score_(hs, pred_cs)  \n",
       "0                 -0.545166  \n",
       "\n",
       "[1 rows x 33 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = pd.read_csv('/home/ameyh/cs-eval/final_data/final_dataset_metrics_computed.csv')\n",
    "df_final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique hate speech: (2009,)\n",
      "Total number of unique counter speech: (4461,)\n",
      "Total number of unique predicted counter speech: (13843,)\n",
      "Total number of datapoints (14508, 33)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of unique hate speech: {df_final['hatespeech'].unique().shape}\")\n",
    "print(f\"Total number of unique counter speech: {df_final['counterspeech'].unique().shape}\")\n",
    "print(f\"Total number of unique predicted counter speech: {df_final['predicted_counterspeech'].unique().shape}\")\n",
    "print(f\"Total number of datapoints {df_final.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aggressiveness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>coherence_score</th>\n",
       "      <th>suitableness_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GPS</th>\n",
       "      <td>1.068627</td>\n",
       "      <td>4.210240</td>\n",
       "      <td>2.668845</td>\n",
       "      <td>2.296296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT3.5_FS</th>\n",
       "      <td>1.053942</td>\n",
       "      <td>4.667704</td>\n",
       "      <td>3.558437</td>\n",
       "      <td>2.601660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT3.5_ZS</th>\n",
       "      <td>1.050411</td>\n",
       "      <td>4.768020</td>\n",
       "      <td>3.951414</td>\n",
       "      <td>2.732208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QUARC</th>\n",
       "      <td>1.002650</td>\n",
       "      <td>4.812426</td>\n",
       "      <td>4.037986</td>\n",
       "      <td>2.808598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dialoGPT</th>\n",
       "      <td>0.986292</td>\n",
       "      <td>4.624743</td>\n",
       "      <td>3.661755</td>\n",
       "      <td>2.796779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           aggressiveness_score  relevance_score  coherence_score  \\\n",
       "source                                                              \n",
       "GPS                    1.068627         4.210240         2.668845   \n",
       "GPT3.5_FS              1.053942         4.667704         3.558437   \n",
       "GPT3.5_ZS              1.050411         4.768020         3.951414   \n",
       "QUARC                  1.002650         4.812426         4.037986   \n",
       "dialoGPT               0.986292         4.624743         3.661755   \n",
       "\n",
       "           suitableness_score  \n",
       "source                         \n",
       "GPS                  2.296296  \n",
       "GPT3.5_FS            2.601660  \n",
       "GPT3.5_ZS            2.732208  \n",
       "QUARC                2.808598  \n",
       "dialoGPT             2.796779  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average scores over each model\n",
    "df_grouped = df_final[['source', 'aggressiveness_score', 'relevance_score', 'coherence_score', 'suitableness_score']].groupby('source').mean()\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x900 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Heatmap (correleation) between different metrics\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('/home/ameyh/cs-eval/final_data/final_dataset_metrics_computed.csv')\n",
    "gold_cols = ['aggressiveness_score', 'relevance_score', 'coherence_score', 'suitableness_score']\n",
    "automated_metric_cols = ['']\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr = df[gold_cols].corr()\n",
    "\n",
    "# Create a mask to display only one triangle of the matrix\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-annotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "Krippendorff's alpha: 0.3999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import krippendorff\n",
    "\n",
    "def calculate_krippendorff_alpha(data):\n",
    "    # Calculate Krippendorff's alpha\n",
    "    alpha = krippendorff.alpha(reliability_data=data)\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "# Toy data: 5 annotators (rows) and 5 data points (columns)\n",
    "data = np.array([\n",
    "    [1, 2, 3, 2, 1],\n",
    "    [2, 3, 4, 3, 2],\n",
    "    [3, 4, 5, 4, 3],\n",
    "    [2, 3, 4, 3, 2],\n",
    "    [1, 2, 3, 2, 1],\n",
    "])\n",
    "print(data.shape)\n",
    "\n",
    "alpha = calculate_krippendorff_alpha(data)\n",
    "print(f\"Krippendorff's alpha: {alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>counterspeech</th>\n",
       "      <th>predicted_counterspeech</th>\n",
       "      <th>csType</th>\n",
       "      <th>source</th>\n",
       "      <th>prediction_(prompt_aggressiveness_score)_(gpt-4)</th>\n",
       "      <th>prediction_(prompt_relevance_score)_(gpt-4)</th>\n",
       "      <th>uuid</th>\n",
       "      <th>prediction_(prompt_aggressiveness_score)_(gpt3.5-turbo)</th>\n",
       "      <th>prediction_(prompt_coherence_score)_(gpt-3.5-turbo)</th>\n",
       "      <th>...</th>\n",
       "      <th>relevance_score_annotator_1</th>\n",
       "      <th>relevance_score_annotator_2</th>\n",
       "      <th>relevance_score_annotator_3</th>\n",
       "      <th>relevance_score_annotator_4</th>\n",
       "      <th>relevance_score_annotator_5</th>\n",
       "      <th>suitableness_score_annotator_1</th>\n",
       "      <th>suitableness_score_annotator_2</th>\n",
       "      <th>suitableness_score_annotator_3</th>\n",
       "      <th>suitableness_score_annotator_4</th>\n",
       "      <th>suitableness_score_annotator_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Every Muslim is a potential terrorist, unfit t...</td>\n",
       "      <td>I understand if you have concerns about Muslim...</td>\n",
       "      <td>It is crucial to deal with this issue with fee...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>GPT3.5_ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e3eb1ff9-eee0-4d81-b953-02ee79e7364f</td>\n",
       "      <td>{'Aggressiveness': '1'}</td>\n",
       "      <td>{'Argument Coherence': '5'}</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          hatespeech  \\\n",
       "0  Every Muslim is a potential terrorist, unfit t...   \n",
       "\n",
       "                                       counterspeech  \\\n",
       "0  I understand if you have concerns about Muslim...   \n",
       "\n",
       "                             predicted_counterspeech    csType     source  \\\n",
       "0  It is crucial to deal with this issue with fee...  Positive  GPT3.5_ZS   \n",
       "\n",
       "  prediction_(prompt_aggressiveness_score)_(gpt-4)  \\\n",
       "0                                              NaN   \n",
       "\n",
       "  prediction_(prompt_relevance_score)_(gpt-4)  \\\n",
       "0                                         NaN   \n",
       "\n",
       "                                   uuid  \\\n",
       "0  e3eb1ff9-eee0-4d81-b953-02ee79e7364f   \n",
       "\n",
       "  prediction_(prompt_aggressiveness_score)_(gpt3.5-turbo)  \\\n",
       "0                            {'Aggressiveness': '1'}        \n",
       "\n",
       "  prediction_(prompt_coherence_score)_(gpt-3.5-turbo)  ...  \\\n",
       "0                        {'Argument Coherence': '5'}   ...   \n",
       "\n",
       "  relevance_score_annotator_1 relevance_score_annotator_2  \\\n",
       "0                           5                           4   \n",
       "\n",
       "   relevance_score_annotator_3  relevance_score_annotator_4  \\\n",
       "0                            5                            5   \n",
       "\n",
       "   relevance_score_annotator_5  suitableness_score_annotator_1  \\\n",
       "0                            4                               4   \n",
       "\n",
       "   suitableness_score_annotator_2  suitableness_score_annotator_3  \\\n",
       "0                               2                               2   \n",
       "\n",
       "   suitableness_score_annotator_4  suitableness_score_annotator_5  \n",
       "0                               3                               4  \n",
       "\n",
       "[1 rows x 59 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_500 = pd.read_csv('/home/ameyh/cs-eval/annotations/annotations_first_pass_500.csv')\n",
    "df_500.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's alpha (relevance score): 0.5521815579452288\n",
      "Krippendorff's alpha (aggressiveness): 0.5496895261029692\n",
      "Krippendorff's alpha (coherence): 0.5610619896126403\n",
      "Krippendorff's alpha (suitableness): 0.4708653346902323\n",
      "Average Krippendorff's alpha: 0.5334496020877677\n"
     ]
    }
   ],
   "source": [
    "data = df_500[['relevance_score_annotator_1','relevance_score_annotator_2','relevance_score_annotator_3','relevance_score_annotator_4','relevance_score_annotator_5']].values.T\n",
    "alpha1 = calculate_krippendorff_alpha(data)\n",
    "print(f\"Krippendorff's alpha (relevance score): {alpha1}\")\n",
    "\n",
    "data = df_500[['aggressiveness_score_annotator_1','aggressiveness_score_annotator_2','aggressiveness_score_annotator_3','aggressiveness_score_annotator_4','aggressiveness_score_annotator_5']].values.T\n",
    "alpha2 = calculate_krippendorff_alpha(data)\n",
    "print(f\"Krippendorff's alpha (aggressiveness): {alpha2}\")\n",
    "\n",
    "data = df_500[['coherence_score_annotator_1','coherence_score_annotator_2','coherence_score_annotator_3','coherence_score_annotator_4','coherence_score_annotator_5']].values.T\n",
    "alpha3 = calculate_krippendorff_alpha(data)\n",
    "print(f\"Krippendorff's alpha (coherence): {alpha3}\")\n",
    "\n",
    "data = df_500[['suitableness_score_annotator_1','suitableness_score_annotator_2','suitableness_score_annotator_3','suitableness_score_annotator_4','suitableness_score_annotator_5']].values.T\n",
    "alpha4 = calculate_krippendorff_alpha(data)\n",
    "print(f\"Krippendorff's alpha (suitableness): {alpha4}\")\n",
    "\n",
    "print(f\"Average Krippendorff's alpha: {(alpha1+alpha2+alpha3+alpha4)/4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1562, 59)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>counterspeech</th>\n",
       "      <th>predicted_counterspeech</th>\n",
       "      <th>csType</th>\n",
       "      <th>source</th>\n",
       "      <th>prediction_(prompt_aggressiveness_score)_(gpt-4)</th>\n",
       "      <th>prediction_(prompt_relevance_score)_(gpt-4)</th>\n",
       "      <th>uuid</th>\n",
       "      <th>prediction_(prompt_aggressiveness_score)_(gpt3.5-turbo)</th>\n",
       "      <th>prediction_(prompt_coherence_score)_(gpt-3.5-turbo)</th>\n",
       "      <th>...</th>\n",
       "      <th>aggressiveness_score_annotator_1</th>\n",
       "      <th>aggressiveness_score_annotator_2</th>\n",
       "      <th>aggressiveness_score_annotator_3</th>\n",
       "      <th>aggressiveness_score_annotator_4</th>\n",
       "      <th>aggressiveness_score_annotator_5</th>\n",
       "      <th>coherence_score_annotator_1</th>\n",
       "      <th>coherence_score_annotator_2</th>\n",
       "      <th>coherence_score_annotator_3</th>\n",
       "      <th>coherence_score_annotator_4</th>\n",
       "      <th>coherence_score_annotator_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muslims have no place in a modern democracy li...</td>\n",
       "      <td>Isn't divesity essential to a modern democracy?</td>\n",
       "      <td>I understand that you have a perspective on Mu...</td>\n",
       "      <td>Questioning</td>\n",
       "      <td>GPT3.5_ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>165a7d8b-c325-4631-91b7-2bf5082a1054</td>\n",
       "      <td>{'Aggressiveness': '1'}</td>\n",
       "      <td>{'Argument Coherence': '1'}</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          hatespeech  \\\n",
       "0  Muslims have no place in a modern democracy li...   \n",
       "\n",
       "                                     counterspeech  \\\n",
       "0  Isn't divesity essential to a modern democracy?   \n",
       "\n",
       "                             predicted_counterspeech       csType     source  \\\n",
       "0  I understand that you have a perspective on Mu...  Questioning  GPT3.5_ZS   \n",
       "\n",
       "  prediction_(prompt_aggressiveness_score)_(gpt-4)  \\\n",
       "0                                              NaN   \n",
       "\n",
       "  prediction_(prompt_relevance_score)_(gpt-4)  \\\n",
       "0                                         NaN   \n",
       "\n",
       "                                   uuid  \\\n",
       "0  165a7d8b-c325-4631-91b7-2bf5082a1054   \n",
       "\n",
       "  prediction_(prompt_aggressiveness_score)_(gpt3.5-turbo)  \\\n",
       "0                            {'Aggressiveness': '1'}        \n",
       "\n",
       "  prediction_(prompt_coherence_score)_(gpt-3.5-turbo)  ...  \\\n",
       "0                        {'Argument Coherence': '1'}   ...   \n",
       "\n",
       "  aggressiveness_score_annotator_1 aggressiveness_score_annotator_2  \\\n",
       "0                                1                                1   \n",
       "\n",
       "   aggressiveness_score_annotator_3  aggressiveness_score_annotator_4  \\\n",
       "0                                 1                                 1   \n",
       "\n",
       "   aggressiveness_score_annotator_5  coherence_score_annotator_1  \\\n",
       "0                                 1                            2   \n",
       "\n",
       "   coherence_score_annotator_2  coherence_score_annotator_3  \\\n",
       "0                            2                            1   \n",
       "\n",
       "   coherence_score_annotator_4  coherence_score_annotator_5  \n",
       "0                            2                            1  \n",
       "\n",
       "[1 rows x 59 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1500 = pd.read_csv('/home/ameyh/cs-eval/annotations/annotations_second_pass_1500_.csv')\n",
    "print(df_1500.shape)\n",
    "df_1500.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's alpha (relevance score): 0.802126254551246\n",
      "Krippendorff's alpha (aggressiveness): 0.7097725372790129\n",
      "Krippendorff's alpha (coherence): 0.7326519605262378\n",
      "Krippendorff's alpha (suitableness): 0.6853627258147301\n",
      "Average Krippendorff's alpha: 0.7324783695428067\n"
     ]
    }
   ],
   "source": [
    "data = df_1500[['relevance_score_annotator_1','relevance_score_annotator_2','relevance_score_annotator_3','relevance_score_annotator_4','relevance_score_annotator_5']].values.T\n",
    "alpha1 = calculate_krippendorff_alpha(data)\n",
    "print(f\"Krippendorff's alpha (relevance score): {alpha1}\")\n",
    "\n",
    "data = df_1500[['aggressiveness_score_annotator_1','aggressiveness_score_annotator_2','aggressiveness_score_annotator_3','aggressiveness_score_annotator_4','aggressiveness_score_annotator_5']].values.T\n",
    "alpha2 = calculate_krippendorff_alpha(data)\n",
    "print(f\"Krippendorff's alpha (aggressiveness): {alpha2}\")\n",
    "\n",
    "data = df_1500[['coherence_score_annotator_1','coherence_score_annotator_2','coherence_score_annotator_3','coherence_score_annotator_4','coherence_score_annotator_5']].values.T\n",
    "alpha3 = calculate_krippendorff_alpha(data)\n",
    "print(f\"Krippendorff's alpha (coherence): {alpha3}\")\n",
    "\n",
    "data = df_1500[['suitableness_score_annotator_1','suitableness_score_annotator_2','suitableness_score_annotator_3','suitableness_score_annotator_4','suitableness_score_annotator_5']].values.T\n",
    "alpha4 = calculate_krippendorff_alpha(data)\n",
    "print(f\"Krippendorff's alpha (suitableness): {alpha4}\")\n",
    "\n",
    "print(f\"Average Krippendorff's alpha: {(alpha1+alpha2+alpha3+alpha4)/4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7926, 75)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('/home/amey/depository/cs-eval/data/annotations/dataset.csv')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "annotator\n",
       "(amey,osho,apoorv)_mean    1500\n",
       "(shaily,rishabh)           1000\n",
       "(aswini)                    400\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.annotator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5627"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['predicted_counterspeech'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_counterspeech\n",
       "It is important to promote inclusivity and respect for all individuals, regardless of their religious background.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       17\n",
       "Generalizing about Jews based on stereotypes is unfair and perpetuates harmful biases. Let's promote dialogue and empathy among all people.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             16\n",
       "Labeling Jews with derogatory terms is hurtful and disrespectful. Let's promote acceptance and support for people of all backgrounds.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   16\n",
       "Jews, like any other group, consist of individuals with diverse backgrounds and beliefs. Let's reject harmful stereotypes and foster understanding and cooperation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     16\n",
       "Spreading hateful stereotypes about Jews only serves to fuel discrimination and division. Let's strive for unity and mutual respect among all people.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   16\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ..\n",
       "Actually, women are not childlike and are fully capable of serious thought and emotion. This statement is based on a stereotype that undermines the intelligence and emotional depth of women. In reality, women have made significant contributions to various fields, including science, politics, and the arts, proving their ability to engage in complex and critical thinking. Additionally, women experience a wide range of emotions just like men do, and it is unfair to dismiss their emotional intelligence. It is important to challenge and debunk such stereotypes to promote equality and respect for all individuals, regardless of their gender.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       1\n",
       "I strongly denounce this statement as a gross misrepresentation of Islam and a perpetuation of harmful stereotypes. It is essential to recognize that Islam, like any other religion, is a complex and diverse faith that cannot be reduced to a single narrative. Painting Islam as inherently violent and intolerant is not only factually incorrect but also contributes to the marginalization and discrimination faced by Muslim communities.It is crucial to understand that acts of violence and intolerance committed by individuals or groups claiming to be Muslims do not represent the teachings or principles of Islam as a whole. Just as with any religion, there are extremists who distort and manipulate religious teachings for their own agenda. However, it is unfair and unjust to attribute the actions of a few to an entire religion and its followers.Promoting peace, compassion, and justice are central tenets of Islam. The Quran emphasizes the importance of peaceful coexistence, respect for others, and the pursuit of justice. Muslims around the world strive to live by these principles and contribute positively to their communities.By perpetuating stereotypes and spreading misinformation about Islam, we only deepen divisions and foster hatred. It is our responsibility to challenge such narratives and promote understanding, empathy, and respect. Let us strive for a world where we can appreciate the diversity of religious beliefs and work     1\n",
       "All Muslims are terrorists. This statement is not only offensive but also perpetuates harmful stereotypes and promotes Islamophobia. It is important to recognize that terrorism has no religion, and it is unfair and unjust to generalize an entire group of people based on the actions of a few individuals.Denouncing this statement, I want to make it clear that categorizing all Muslims as terrorists is a manifestation of prejudice and bigotry. Such rhetoric only serves to further divide communities and perpetuate fear and hatred. It is crucial to challenge these harmful narratives and promote understanding and empathy instead.Muslims, like any other religious or ethnic group, are diverse and encompass a wide range of beliefs, values, and contributions to society. They are doctors, teachers, artists, engineers, and so much more. By reducing them to a single stereotype, we deny their individuality and humanity.It is essential to educate ourselves and others about Islam and the rich cultural heritage of Muslim communities. By doing so, we can break down stereotypes and foster a more inclusive and tolerant society. Let us reject hate speech and work towards building bridges of understanding and respect among all people, regardless of their religious background.                                                                                                                                                                                1\n",
       "I strongly denounce and categorize this statement as a manifestation of xenophobia and Islamophobia. It is deeply troubling to witness such prejudiced and divisive rhetoric being perpetuated. Muslims, like any other religious or cultural group, have every right to be a part of the modern European community. They contribute to the diversity, richness, and progress of our societies.This statement not only undermines the principles of inclusivity and equality that our modern European community should uphold, but it also perpetuates harmful stereotypes and generalizations. It is essential to recognize that Islam is a diverse and multifaceted religion, and Muslims are individuals with their own unique experiences, beliefs, and contributions.By promoting this kind of hateful narrative, we are only furthering division and marginalization within our communities. It is crucial for us to challenge and confront such harmful rhetoric, fostering an environment of understanding, respect, and acceptance.I urge everyone to reject this discriminatory viewpoint and instead embrace the values of tolerance, diversity, and unity. Let us work together to build a modern European community that celebrates and embraces people of all backgrounds, including Muslims.                                                                                                                                                                                              1\n",
       "Can you provide evidence to support your claim that refugees are a problem in our country? What specific issues are you referring to? Are you aware of the contributions that refugees have made to our society? Have you considered the reasons why people become refugees and the challenges they face? Are you suggesting that we should turn our backs on those in need? What solutions do you propose to address the situation in a more compassionate and effective manner?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1\n",
       "Name: count, Length: 5626, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['predicted_counterspeech'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7926"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280.0, 627)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1400 / 5,627"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
