{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from together import Together\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"sk-proj-SWPkbjlG7ZO9tAKANL69T3BlbkFJ3iiUUZHszD9AuQnEjhLE\"\n",
    "os.environ['TOGETHER_API_KEY'] = \"90e216b11fbd62f9b0ce3a9666cbcb134a116857b3b62f95b6643bde91feb364\"\n",
    "\n",
    "try:\n",
    "    openai_client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY'] \n",
    "    )\n",
    "\n",
    "    togetherai_client = Together(\n",
    "        api_key=os.environ.get(\"TOGETHER_API_KEY\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Please specify your OPENAI and TOGETHER-AI keys for inference\")\n",
    "\n",
    "def predict(\n",
    "      prompt, \n",
    "      system_description,\n",
    "      use_temperature_samping=False,\n",
    "      model_name=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    #   model_name=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "):\n",
    "    temperature = 0.5 if not use_temperature_samping else random.uniform(0, 0.5)\n",
    "\n",
    "    if 'gpt' in model_name:\n",
    "        response = openai_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        temperature=temperature,\n",
    "        top_p=1,\n",
    "        response_format = { \"type\": \"json_object\"},\n",
    "        messages = [\n",
    "                {\"role\": \"system\", \"content\": system_description},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        return content\n",
    "    \n",
    "    else:\n",
    "        response = togetherai_client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_description},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=1024,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=[\"</s>\"],\n",
    "            logprobs=False,\n",
    "            n=1\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        return content\n",
    "        \n",
    "\n",
    "def predict_openai(\n",
    "      prompt, \n",
    "      system_description,\n",
    "      use_temperature_samping=False,\n",
    "      model_name=\"gpt-4o\"\n",
    "\n",
    "):\n",
    "    temperature = 0.5 if not use_temperature_samping else random.uniform(0, 0.5)\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    temperature=temperature,\n",
    "    top_p=1,\n",
    "    response_format = { \"type\": \"json_object\"},\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": system_description},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content\n",
    "\n",
    "def predict_togetherai(\n",
    "        input_prompt, \n",
    "        system_description, \n",
    "        use_temperature_samping=False,\n",
    "        model_name=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    ):\n",
    "\n",
    "    temperature = 0.5 if not use_temperature_samping else random.uniform(0, 0.5)\n",
    "\n",
    "    response = togetherai_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_description},\n",
    "            {\"role\": \"user\", \"content\": input_prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=1024,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "        logprobs=False,\n",
    "        n=1\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from cleantext import clean\n",
    "from loguru import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dashed_line = \"- \" * 50\n",
    "\n",
    "def clean_text(text: str, min_len=4) -> str:\n",
    "    \"\"\"\n",
    "    Returns cleaned text (post/title) by removing extra whitespaces, unprintable characters, recurring punctions, & urls\n",
    "\n",
    "    Before:\n",
    "\n",
    "    My ðŸ¤ [ndad] (https://np.reddit.com/r/raisedbynarcissists/comments/4puzqb/if_ndad_shows_up_to_my_graduation_im_going_to/) and I have been estranged for about 4-5 months now. Since the estrangement, I have noticed feeling this desire to tighten my abdominal muscles when I remember something my ndad did or didn't do that led me to feel so much pain as a result of feeling angry, hurt or depressed by all the hurt he has caused me.\n",
    "\n",
    "    The ¤ only other time I think of tightening my abdominal muscles is during exercise because I remember when I used exercise videos, the instructor would instruct such...!\n",
    "\n",
    "    I'm taking medications ([at the very least, I am trying](https://np.reddit.com/r/Anger/comments/4rjatz/new_doctor_wants_me_to_go_off_antidepressants/)) and am going to therapy so I hope I am handling this pretty well, just wanted to get second opinions.\n",
    "\n",
    "    Should I be concerned about this.......? Do you have any idea why one would desire to tighten abdominal muscles in this situation or in situations such as these......!?\n",
    "    Ÿ\n",
    "\n",
    "\n",
    "    *********************************************************************************************************************************\n",
    "\n",
    "    After:\n",
    "    my [ndad] (<url>) and i have been estranged for about 4-5 months now. since the estrangement, i have noticed feeling this desire to tighten my abdominal muscles when i remember something my ndad did or didn't do that led me to feel so much pain as a result of feeling angry, hurt or depressed by all the hurt he has caused me. the  only other time i think of tightening my abdominal muscles is during exercise because i remember when i used exercise videos, the instructor would instruct such! i'm taking medications ([at the very least, i am trying](<url>)) and am going to therapy so i hope i am handling this pretty well, just wanted to get second opinions. should i be concerned about this? do you have any idea why one would desire to tighten abdominal muscles in this situation or in situations such as these?\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    punc = \"\"\"()-[]{};:'\"\\<>/@#$%^&*_~\"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        logger.debug(ValueError(f\"encountered invalid format string: {text}\"))\n",
    "        return None\n",
    "\n",
    "    text = re.sub(\n",
    "        f\"[^{re.escape(string.printable)}]\",\n",
    "        \"\",\n",
    "        re.sub(\n",
    "            r\"[\\?\\.\\!]+(?=[\\?\\.\\!])\",\n",
    "            \"\",\n",
    "            clean(\n",
    "                text,\n",
    "                fix_unicode=False,\n",
    "                to_ascii=False,\n",
    "                lower=False,\n",
    "                no_line_breaks=True,\n",
    "                no_urls=True,\n",
    "                no_emails=True,\n",
    "                no_punct=False,\n",
    "                no_emoji=True,\n",
    "                no_currency_symbols=True,\n",
    "                lang=\"en\",\n",
    "                replace_with_url=\"\",\n",
    "                replace_with_email=\"\",\n",
    "            ),\n",
    "        ),\n",
    "    ).strip()\n",
    "\n",
    "    for _ in punc:\n",
    "        text = text.replace(_, \"\")\n",
    "    text = text.replace(f\". .\", \". \").replace(\"  \", \" \").replace('\"','')\n",
    "    text = text.strip()\n",
    "\n",
    "    if len(text) < min_len:\n",
    "        return \"<EMPTY_TEXT>\"\n",
    "    else:\n",
    "        return text.strip()\n",
    "\n",
    "\n",
    "def load_json(fpath: str):\n",
    "    if not fpath.endswith(\".json\"):\n",
    "        raise ValueError(f\"{fpath} not a json file\")\n",
    "\n",
    "    with open(fpath, \"r\") as fp:\n",
    "        return json.load(fp)\n",
    "\n",
    "\n",
    "def save_json(data, fpath: str):\n",
    "    if not fpath.endswith(\".json\"):\n",
    "        raise ValueError(f\"{fpath} not a json file\")\n",
    "\n",
    "    with open(fpath, \"w\") as fp:\n",
    "        return json.dump(data, fp)\n",
    "\n",
    "\n",
    "def split_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    test_size: float,\n",
    "    id_col: str,\n",
    "    stratify_col: str,\n",
    "    random_state: int,\n",
    "):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        df[id_col].values.tolist(),\n",
    "        df[stratify_col].values.tolist(),\n",
    "        stratify=df[stratify_col].values.tolist(),\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    df_train = df[df[id_col].isin(x_train)]\n",
    "    df_test = df[df[id_col].isin(x_test)]\n",
    "    df_train = df_train.reset_index()\n",
    "    df_test = df_test.reset_index()\n",
    "\n",
    "    logger.debug(\n",
    "        f\"\\nTrain set: {df_train.shape}\\n{df_train[stratify_col].value_counts()}\"\n",
    "    )\n",
    "    logger.debug(f\"\\nTest Set: {df_test.shape}\\n{df_test[stratify_col].value_counts()}\")\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def search_best_match(search_text: str, df: pd.DataFrame, column: str):\n",
    "    # Load the pre-trained sentence transformer model\n",
    "    model = SentenceTransformer(\"distilbert-base-nli-stsb-mean-tokens\")\n",
    "\n",
    "    # Compute the embeddings for the search text and the DataFrame column\n",
    "    search_text_embedding = model.encode([search_text])\n",
    "    column_embeddings = model.encode(df[column].tolist())\n",
    "\n",
    "    # Compute the cosine similarity between the embeddings\n",
    "    similarities = cosine_similarity(search_text_embedding, column_embeddings)\n",
    "\n",
    "    # Find the index of the best match\n",
    "    best_match_index = similarities.argmax()\n",
    "\n",
    "    # Return the best match\n",
    "    return best_match_index, df.iloc[best_match_index]\n",
    "\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "from utils import dashed_line\n",
    "import string\n",
    "\n",
    "def clean_str(input_string)->str:\n",
    "    input_string = input_string.strip() # Remove extra line spaces\n",
    "    input_string = input_string.replace('\\n',' </s> ') # Replace line spaces with /s<>\n",
    "    input_string = input_string.replace('  ',' ')\n",
    "    input_string = input_string.replace(\"\\\\\",' ')\n",
    "    input_string = input_string.replace(\"/\",' ')\n",
    "    input_string = input_string.lower() # Lower all characters.\n",
    "    return input_string\n",
    "\n",
    "def postprocess_llm_pred(llm_prediction:str, key=\"\") -> str:\n",
    "    \"\"\"\n",
    "    Function to parse the right answer from noisy LLM prediction\n",
    "    \"\"\"\n",
    "    answer = '<None>'\n",
    "    if not isinstance(llm_prediction, str) or len(llm_prediction)<1 or llm_prediction == \"{}\": # Return default value in case prediction == None or empty.\n",
    "        return answer\n",
    "    else:\n",
    "        if \"Question:\" in llm_prediction: # For models like llama, output is both the prompt and prediction.\n",
    "            llm_prediction = llm_prediction.split(\"Answer:\")[-1]\n",
    "        llm_prediction = clean_str(llm_prediction)\n",
    "\n",
    "        if '{' in llm_prediction and '}' in llm_prediction: # Check if any json / dict object is present within the prediction.\n",
    "            dict_str = extract_json(llm_prediction) # Extract dict object.\n",
    "            if dict_str is not None: # Succefully extracted dict from string.\n",
    "                dict_obj = str_to_dict(dict_str)\n",
    "                if isinstance(dict_obj, dict) and len(dict_obj) > 0: # Succesfully converted string to dict\n",
    "                    try:\n",
    "                        key = list(dict_obj.keys())[0]\n",
    "                        key_ = remove_punctuation(key)\n",
    "                        key_ = key_.strip()\n",
    "                        key_ = key_.lower()\n",
    "                        dict_obj[key_] = dict_obj[key]\n",
    "                    except:\n",
    "                        print(dict_obj)\n",
    "                    if not 'answer' in dict_obj:\n",
    "                        print(\n",
    "                           f\"Encountered a prediction with dict object withou 'answer' key:\\n{llm_prediction}\"\n",
    "                        )\n",
    "                        return remove_punctuation(llm_prediction)\n",
    "\n",
    "                    answer = dict_obj['answer']\n",
    "                    answer = remove_punctuation(answer)\n",
    "                    return answer\n",
    "        \n",
    "        else: # No json / dict object in prediction.\n",
    "            answer = remove_punctuation(llm_prediction)\n",
    "            return answer\n",
    "        \n",
    "    return answer\n",
    "\n",
    "def extract_json(input_string):\n",
    "    start_index = input_string.find('{')\n",
    "    end_index = input_string.find('}')\n",
    "    \n",
    "    if start_index == -1 or end_index == -1 or end_index <= start_index:\n",
    "        return None\n",
    "    else:\n",
    "        dict_str = \"{\" + input_string[start_index+1:end_index] + \"}\"\n",
    "        return dict_str\n",
    "\n",
    "def str_to_dict(input_string):\n",
    "    input_string = input_string.strip()\n",
    "    if len(input_string.split(':')) == 2: # dict object in string and not a set object\n",
    "        input_string = input_string.split(':')\n",
    "        key = input_string[0]\n",
    "        key = key.lower().strip().replace('\"','').replace(\"'\",'').replace(\"{\",\"\")\n",
    "        key = f'\"{key}\"'\n",
    "\n",
    "        value = input_string[1]\n",
    "        value = value.lower().strip().replace('\"','').replace(\"'\",'').replace(\"}\",\"\")\n",
    "        value = f'\"{value}\"'\n",
    "\n",
    "        input_string = \"{\" + key + \":\" + value + \"}\"\n",
    "\n",
    "    try:\n",
    "        dict_obj = json.loads(input_string)\n",
    "        if isinstance(dict_obj, set):\n",
    "            dict_obj = {'answer': list(dict_obj)[0]}\n",
    "        return dict_obj\n",
    "    except:\n",
    "        try:\n",
    "            dict_obj = ast.literal_eval(input_string)\n",
    "            if isinstance(dict_obj, set):\n",
    "                dict_obj = {'answer': list(dict_obj)[0]}\n",
    "            return dict_obj\n",
    "        except:\n",
    "            print(f\"Could not convert the following string to dict:\\n{input_string}\")\n",
    "            print(dashed_line)\n",
    "            return None\n",
    "                \n",
    "def remove_punctuation(input_string, replace_by=\"\"):\n",
    "    # return re.sub(r'[^\\w\\s]', ' ', input_string)\n",
    "    punctuations = string.punctuation\n",
    "    punctuations.replace('<','')\n",
    "    punctuations.replace('>','')\n",
    "    punctuations.replace(\"'\",'')\n",
    "    return f'{replace_by}'.join(char for char in input_string if char not in punctuations)\n",
    "\n",
    "\n",
    "def get_val(d):\n",
    "    \"\"\"\n",
    "    Returns the value from a dictionary containing a single key-value pair.\n",
    "    \n",
    "    Args:\n",
    "    d (dict): A dictionary with a single key-value pair.\n",
    "    \n",
    "    Returns:\n",
    "    The value of the single key-value pair in the dictionary.\n",
    "    \"\"\"\n",
    "    assert isinstance(d, str)\n",
    "    d = ast.literal_eval(d)\n",
    "    return float(next(iter(d.values())))\n",
    "\n",
    "\n",
    "def extract_score(input_string: str, key='Relevance') -> float:\n",
    "    if not isinstance(input_string, str):\n",
    "        print(\n",
    "            f\"Invalid input: {input_string}\"\n",
    "        )\n",
    "        return -1\n",
    "    \"\"\"\n",
    "    try:\n",
    "        input = ast.literal_eval(input)\n",
    "        score = input[key]\n",
    "        score = score.strip().replace(' ','')\n",
    "        score = float(score)\n",
    "        return score\n",
    "    except:\n",
    "        return -1\n",
    "    \"\"\"\n",
    "     # Use regex to find the number in the string\n",
    "    match = re.search(r'\\d+', input_string)\n",
    "    \n",
    "    # If a number is found, convert it to float and return it\n",
    "    if match:\n",
    "        return float(match.group())\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def extract_content_from_response(response):\n",
    "    try:\n",
    "        return response.choices[0].message.content\n",
    "    except:\n",
    "        print(response)\n",
    "\n",
    "\n",
    "def postprocess_llm_response(response):\n",
    "    if isinstance(response, dict):\n",
    "        return list(response.keys())[0]\n",
    "    else:\n",
    "        try:\n",
    "            x = json.loads(response)\n",
    "            return list(x.keys())[0]\n",
    "        except:\n",
    "            return str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt templates\n",
    "\n",
    "common_input_prompt = lambda hs, cs: f\"\"\"\n",
    "Hate Speech: \"{hs}\"\\nCounterspeech: \"{cs}\"\n",
    "\"\"\".strip()\n",
    "\n",
    "instruction_prompt = f\"\"\"\n",
    "You will be given one counterspeech (also called as counter-argument or counter-narrative) written for a hateful statement (hate speech). Your\n",
    "task is to rate the counterspeech on one metric. Please make sure you read and understand these instructions carefully.\n",
    "\"\"\".strip()\n",
    "\n",
    "evaluation_criteria_relevance = f\"\"\"\n",
    "Contextual Relevance (1-5) - This dimension evaluates whether the counterspeech addresses the central topic, theme subject of the given hate\n",
    "speech.\n",
    "\"\"\".strip()\n",
    "\n",
    "evaluation_criteria_aggressiveness = f\"\"\"\n",
    "Aggressiveness (1-5) - This metric assesses the degree of confrontational or inflammatory content in the counterspeech. It considers factors such\n",
    "as abusive language, intensity of disagreement, tone, engagement style, and the presence of personal attacks, with higher scores indicating\n",
    "greater aggression.\n",
    "\"\"\".strip()\n",
    "\n",
    "evaluation_criteria_coherence = f\"\"\"\n",
    "Argument Coherence (1-5) - This metric assesses how logically and smoothly the ideas or arguments within the counterspeech connect and\n",
    "flow. A coherent counterspeech will present its arguments in an organized manner, making it easy for the reader to follow and understand the\n",
    "counter-narrative being presented.\n",
    "\"\"\".strip()\n",
    "\n",
    "evaluation_criteria_suitableness = f\"\"\"\n",
    "Suitableness (1-3) - This metric measures the likelihood of an annotator choosing a given counterspeech for direct use (without editing) in a\n",
    "real scenario against online hate speech. This assessment considers the counterspeech's suitability, appropriateness, and potential impact on a\n",
    "reader in a real-world context.\n",
    "\"\"\".strip()\n",
    "\n",
    "# ----------------------------------------------------------------------- #\n",
    "\n",
    "relevance_score_prompt_zs = f\"\"\"\n",
    "{instruction_prompt}\n",
    "\n",
    "Evaluation Criteria:\n",
    "{evaluation_criteria_relevance}\n",
    "\n",
    "Ensure that the response is STRICTLY in JSON format as {{\"Relevance\": \"\"}}\n",
    "\"\"\".strip()\n",
    "\n",
    "coherence_score_prompt_zs = f\"\"\"\n",
    "{instruction_prompt}\n",
    "\n",
    "Evaluation Criteria:\n",
    "{evaluation_criteria_coherence}\n",
    "\n",
    "Ensure that the response is STRICTLY in JSON format as {{\"Argument Coherence\": \"\"}}.\n",
    "\"\"\".strip()\n",
    "\n",
    "suitableness_score_prompt_zs = f\"\"\"\n",
    "{instruction_prompt}\n",
    "\n",
    "Evaluation Criteria:\n",
    "{evaluation_criteria_suitableness}\n",
    "\n",
    "Ensure that the response is STRICTLY in JSON format as {{\"Suitableness\": \"\"}}.\n",
    "\"\"\".strip()\n",
    "\n",
    "aggressiveness_score_prompt_zs = f\"\"\"\n",
    "{instruction_prompt}\n",
    "\n",
    "Evaluation Criteria:\n",
    "{evaluation_criteria_aggressiveness}\n",
    "\n",
    "Ensure that the response is STRICTLY in JSON format as {{\"Aggressiveness\": \"\"}}.\n",
    "\"\"\".strip()\n",
    "\n",
    "# ----------------------------------------------------------------------- #\n",
    "\n",
    "relevance_score_prompt_cot = lambda evaluation_steps : f\"\"\"\n",
    "{instruction_prompt}\n",
    "\n",
    "Evaluation Criteria:\n",
    "{evaluation_criteria_relevance}\n",
    "\n",
    "Evaluation Steps:\n",
    "{evaluation_steps}\n",
    "\n",
    "Ensure that the response is STRICTLY in JSON format as {{\"Relevance\": \"\"}}\n",
    "\"\"\".strip()\n",
    "\n",
    "coherence_score_prompt_cot = lambda evaluation_steps : f\"\"\"\n",
    "{instruction_prompt}\n",
    "\n",
    "Evaluation Criteria:\n",
    "{evaluation_criteria_coherence}\n",
    "\n",
    "Evaluation Steps:\n",
    "{evaluation_steps}\n",
    "\n",
    "Ensure that the response is STRICTLY in JSON format as {{\"Argument Coherence\": \"\"}}.\n",
    "\"\"\".strip()\n",
    "\n",
    "suitableness_score_prompt_cot = lambda evaluation_steps : f\"\"\"\n",
    "{instruction_prompt}\n",
    "\n",
    "Evaluation Criteria:\n",
    "{evaluation_criteria_suitableness}\n",
    "\n",
    "Evaluation Steps:\n",
    "{evaluation_steps}\n",
    "\n",
    "Ensure that the response is STRICTLY in JSON format as {{\"Suitableness\": \"\"}}.\n",
    "\"\"\".strip()\n",
    "\n",
    "aggressiveness_score_prompt_cot = lambda evaluation_steps : f\"\"\"\n",
    "{instruction_prompt}\n",
    "\n",
    "Evaluation Criteria:\n",
    "{evaluation_criteria_aggressiveness}\n",
    "\n",
    "Evaluation Steps:\n",
    "{evaluation_steps}\n",
    "\n",
    "Ensure that the response is STRICTLY in JSON format as {{\"Aggressiveness\": \"\"}}.\n",
    "\"\"\".strip()\n",
    "\n",
    "# ----------------------------------------------------------------------- #\n",
    "\n",
    "cot_gen_few_shot_tempate = lambda aspect, hs1, cs1, r1, hs2, cs2, r2, hs3, cs3, r3: f\"\"\"\n",
    "Hatespeech: {hs1}\n",
    "Counterspeech: {cs1}\n",
    "Expert Rating ({aspect}): {r1}\n",
    "\n",
    "Hatespeech: {hs2}\n",
    "Counterspeech: {cs2}\n",
    "Expert Rating ({aspect}): {r2}\n",
    "\n",
    "Hatespeech: {hs3}\n",
    "Counterspeech: {cs3}\n",
    "Expert Rating ({aspect}): {r3}\n",
    "\"\"\".strip()\n",
    "\n",
    "candidate_cot_drafting_template = lambda aspect, evaluation_criteria, few_shot_examples : f\"\"\"\n",
    "You will be given sets of in-context examples, each containing a hate speech, a corresponding\n",
    "counterspeech, and an {aspect} rated by a human expert. Your task is to generate a suitable set\n",
    "of Evaluation Steps based on the analysis of these examples. Please make sure you read and\n",
    "understand these instructions carefully.\n",
    "\n",
    "## Examples:\n",
    "{few_shot_examples}\n",
    "\n",
    "## Instruction:\n",
    "Analyze the examples below to identify patterns and factors that influence the {aspect} rating.\n",
    "Then, create a detailed set of steps outlining how to evaluate the {aspect} of counterspeeches.\n",
    "\n",
    "## Evaluation Criteria for {aspect}:\n",
    "{evaluation_criteria}\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "# aspect = \"Aggressiveness\"\n",
    "# few_shot_examples = cot_gen_few_shot_tempate(aspect,\"hs1\", \"cs1\", \"r1\", \"hs2\", \"cs2\", \"r2\", \"hs3\", \"cs3\", \"r3\")\n",
    "# prompt = candidate_cot_drafting_template(aspect, evaluation_criteria_aggressiveness, few_shot_examples)\n",
    "# print(prompt)\n",
    "\n",
    "# ----------------------------------------------------------------------- #\n",
    "\n",
    "cot_error_few_shot_tempate = lambda aspect, hs1, cs1, r1, p1, hs2, cs2, r2, p2, hs3, cs3, r3, p3: f\"\"\"\n",
    "Hatespeech: {hs1}\n",
    "Counterspeech: {cs1}\n",
    "Actual Rating ({aspect}): {r1}\n",
    "Predicted Rating ({aspect}): {p1}\n",
    "\n",
    "Hatespeech: {hs2}\n",
    "Counterspeech: {cs2}\n",
    "Actual Rating ({aspect}): {r2}\n",
    "Predicted Rating ({aspect}): {p2}\n",
    "\n",
    "Hatespeech: {hs3}\n",
    "Counterspeech: {cs3}\n",
    "Actual Rating ({aspect}): {r3}\n",
    "Predicted Rating ({aspect}): {p3}\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "candidate_cot_refinement_template = lambda aspect, candidate_cot, few_shot_examples: f\"\"\"\n",
    "Please refine and improve the chain-of-thought (CoT) evaluation steps used by a large language model in evaluating {aspect} of counterspeech\n",
    "generation.\n",
    "\n",
    "Large language models (LLMs) are powerful neural models that can evaluate the quality of counterspeech generation. However, LLMs may not\n",
    "always agree with human judgments. Please refine the CoT used by LLMs to improve its correlation with human expert scores. To refine the\n",
    "scoring criteria used by the LLM in evaluating the {aspect}, please follow the following instructions step-by-step:\n",
    "1. Carefully read each example, understand each hate speech and its corresponding counterspeech, and get your initial assessment of its\n",
    "quality on {aspect}.\n",
    "2. Compare the test score obtained by the LLM according to the CoT and the ground-truth score from human experts. Please think why the\n",
    "correlation is limited by using the current CoT, and how can you improve the CoT to increase the correlation between LLM’s score and\n",
    "human expert score. If there is a small gap or no gap, this means the CoT work well in this case.\n",
    "3. Read all of the test cases and rethink how you could refine the current CoT based on your observations and analysis. Then, refine the\n",
    "CoT to make it concise, accurate, and consistent with human judgments. When refining the CoT, you can do the following: 1)\n",
    "modification: adjust some parts of the CoT to increase its correlation with the scoring CoT that you think might used by human experts;\n",
    "2) paraphrase: if the CoT is good enough, you can consider paraphrasing it to make more concise and easy to understand; 3) adding\n",
    "aspects or details: if you find some new underlying scoring rules not covered by the current CoT, consider adding them as a new line of\n",
    "injecting to current CoT, but make sure not to make the CoT too long and redundant; 4) calibrate: you can take other methods you think\n",
    "being helpful to improve the correlation with human experts.\n",
    "Please return only your refined criteria without any additional sentences.\n",
    "\n",
    "Old criteria:\n",
    "{candidate_cot}\n",
    "\n",
    "Error Examples:\n",
    "{few_shot_examples}\n",
    "\"\"\".strip()\n",
    "\n",
    "# aspect = \"Aggressiveness\"\n",
    "# candidate_cot = \"Rate 1 as most aggressive and 5 as least aggressive\"\n",
    "# few_shot_examples = cot_error_few_shot_tempate(aspect, \"hs1\", \"cs1\", \"r1\", \"p1\", \"hs2\", \"cs2\", \"r2\", \"p2\", \"hs3\", \"cs3\", \"r3\", \"p3\")\n",
    "# prompt = candidate_cot_refinement_template(aspect, candidate_cot, few_shot_examples)\n",
    "# print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv('/home/amey/depository/cs-eval/data/annotations/dev_set_500.csv')\n",
    "\n",
    "dev_set_gold_relevance_score = []\n",
    "dev_set_gold_coherence_score = []\n",
    "dev_set_gold_aggressiveness_score = []\n",
    "dev_set_gold_suitableness_score = []\n",
    "\n",
    "for i, row in df_dev.iterrows():\n",
    "    dev_set_gold_relevance_score.append(\n",
    "        (row['hatespeech'], row['predicted_counterspeech'], row['relevance_score'])\n",
    "    )\n",
    "    dev_set_gold_coherence_score.append(\n",
    "        (row['hatespeech'], row['predicted_counterspeech'], row['coherence_score'])\n",
    "    )\n",
    "    dev_set_gold_aggressiveness_score.append(\n",
    "        (row['hatespeech'], row['predicted_counterspeech'], row['aggressiveness_score'])\n",
    "    )\n",
    "    dev_set_gold_suitableness_score.append(\n",
    "        (row['hatespeech'], row['predicted_counterspeech'], row['suitableness_score'])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_candidate_cot(criteria, system_description=\"\"):\n",
    "#     prompt = criteria\n",
    "#     return f\"candidate cot {len(prompt)}\"\n",
    "\n",
    "# def generate_candidate_cot_self_refinement(refinement_prompt, system_description=\"\"):\n",
    "#     prompt = refinement_prompt\n",
    "#     return f\"refined candidate cot {len(prompt)}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Toy values for the golden set D*\n",
    "# dev_set_gold = [\n",
    "#     (\"Article A1\", \"Summary S1\", 5),\n",
    "#     (\"Article A2\", \"Summary S2\", 4),\n",
    "#     (\"Article A3\", \"Summary S3\", 3),\n",
    "#     (\"Article A4\", \"Summary S4\", 2),\n",
    "#     (\"Article A5\", \"Summary S5\", 1)\n",
    "# ]\n",
    "\n",
    "dev_set_gold = dev_set_gold_relevance_score\n",
    "aspect = \"Relevance\"\n",
    "evaluation_criteria = evaluation_criteria_relevance\n",
    "\n",
    "# Number of Monte Carlo trials\n",
    "num_trials = 10\n",
    "\n",
    "# Few-shot exemplar size\n",
    "few_shot_size = 3\n",
    "\n",
    "# Define topk\n",
    "topk = 3\n",
    "\n",
    "\n",
    "# Perform Monte Carlo trials\n",
    "criteria_set = []\n",
    "candidate_cot_set = []\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    print(f\"Trial {trial + 1}:\")\n",
    "\n",
    "    # Randomly sample few-shot examples from the golden set\n",
    "    sampled_examples = random.sample(dev_set_gold, few_shot_size)\n",
    "\n",
    "    # Infer criteria based on sampled examples\n",
    "    hs1, cs1, r1 = sampled_examples[0][0], sampled_examples[0][1], sampled_examples[0][2]\n",
    "    hs2, cs2, r2 = sampled_examples[1][0], sampled_examples[1][1], sampled_examples[1][2]\n",
    "    hs3, cs3, r3 = sampled_examples[2][0], sampled_examples[2][1], sampled_examples[2][2]\n",
    "\n",
    "    few_shot_examples = cot_gen_few_shot_tempate(aspect, hs1, cs1, r1, hs2, cs2, r2, hs3, cs3, r3)\n",
    "    criteria = candidate_cot_drafting_template(aspect, evaluation_criteria, few_shot_examples)\n",
    "    \n",
    "    # Store the inferred criteria\n",
    "    criteria_set.append(criteria)\n",
    "\n",
    "    candidate_cot = predict(prompt=criteria, system_description=\"\", use_temperature_samping=True)\n",
    "    print(candidate_cot)\n",
    "    candidate_cot = postprocess_llm_response(candidate_cot)\n",
    "    candidate_cot_set.append(candidate_cot)\n",
    "    \n",
    "    print(f\"Sampled Examples: {sampled_examples}\")\n",
    "    print(f\"Inferred Criteria: {criteria}\")\n",
    "    print(f\"Candidate COT: {candidate_cot}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metric import spearman_correlation, kendall_tau\n",
    "import json\n",
    "\n",
    "\n",
    "def postprocess_llm_response(response):\n",
    "    if isinstance(response, dict):\n",
    "        return list(response.keys())[0]\n",
    "    else:\n",
    "        try:\n",
    "            x = json.loads(response)\n",
    "            return list(x.keys())[0]\n",
    "        except:\n",
    "            return str(response)\n",
    "\n",
    "\n",
    "def evaluate_candidate_cot(aspect, candidate_cot, test_size=10) -> float:\n",
    "    # Returns average correlation scores for each candidate cot.\n",
    "    \n",
    "    print(f\"Total candidates to evaluate: {len(candidate_cot_set)}\")\n",
    "    scoring_prompt = None\n",
    "    actual_scores = []\n",
    "    predicted_scores = []\n",
    "    exemplars = []\n",
    "\n",
    "    if aspect == \"Relevance\":\n",
    "        scoring_prompt = relevance_score_prompt_cot(evaluation_steps=candidate_cot)\n",
    "    elif aspect == \"Aggressiveness\":\n",
    "        scoring_prompt = aggressiveness_score_prompt_cot(evaluation_steps=candidate_cot)\n",
    "    elif aspect == \"Coherence\":\n",
    "        scoring_prompt = coherence_score_prompt_cot(evaluation_steps=candidate_cot)\n",
    "    elif aspect == \"Suitableness\":\n",
    "        scoring_prompt = suitableness_score_prompt_cot(evaluation_steps=candidate_cot)\n",
    "\n",
    "    print(f\"Evaluating on {test_size} datapoints\")\n",
    "    for i, row in tqdm(df_dev[:test_size].iterrows(), desc=\"Getting Predictions\"):\n",
    "        input_prompt = common_input_prompt(row['hatespeech'], row['predicted_counterspeech'])\n",
    "        system_description = scoring_prompt\n",
    "\n",
    "        response = predict(\n",
    "            prompt=input_prompt, \n",
    "            system_description=system_description\n",
    "        )\n",
    "        predicted_score = extract_score(response)\n",
    "        predicted_scores.append(predicted_score)\n",
    "\n",
    "        if aspect == \"Relevance\":\n",
    "            actual_score = row[\"relevance_score\"]\n",
    "        elif aspect == \"Aggressiveness\":\n",
    "            actual_score = row[\"aggressiveness_score\"]\n",
    "        elif aspect == \"Coherence\":\n",
    "            actual_score = row[\"coherence_score\"]\n",
    "        elif aspect == \"Suitableness\":\n",
    "            actual_score = row[\"suitableness_score\"]\n",
    "        \n",
    "        actual_scores.append(actual_score)\n",
    "\n",
    "        exemplars.append(\n",
    "            (row['hatespeech'], row['predicted_counterspeech'], actual_score, predicted_score)\n",
    "        )\n",
    "\n",
    "    spearman_score = spearman_correlation(actual_scores, predicted_scores)\n",
    "    kendalltau_score = kendall_tau(actual_scores, predicted_scores)\n",
    "    mean_score = (spearman_score + kendalltau_score)/2\n",
    "    return (mean_score, exemplars)\n",
    "\n",
    "evaluated_candidates = [(candidate_cot, evaluate_candidate_cot(aspect, candidate_cot)[0], evaluate_candidate_cot(aspect, candidate_cot)[1]) for candidate_cot in candidate_cot_set]\n",
    "evaluated_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "top_candidates = evaluated_candidates[:topk]\n",
    "\n",
    "print(\"Top-performing Criteria:\")\n",
    "for candidate_cot, score, exemplars in top_candidates:\n",
    "    print(f\"Candidate COT: {candidate_cot}, Score: {score}, Exempars: {exemplars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_refinement_of_cot_candidate(candidate_cot, aspect) -> list[str]:\n",
    "    # Performs interative refinement of top candidate via self-assessement prompting, and returns topk candidates with highest correlation\n",
    "    exemplars = candidate_cot[2]\n",
    "    refinement_prompts = []\n",
    "    candidate_cot_set = []\n",
    "\n",
    "    # Number of Monte Carlo trials\n",
    "    num_trials = 3\n",
    "\n",
    "    # Few-shot exemplar size\n",
    "    few_shot_size = 3\n",
    "\n",
    "\n",
    "    for trial in range(num_trials):\n",
    "        print(f\"Trial {trial + 1}:\")\n",
    "\n",
    "        # Randomly sample few-shot examples from the golden set\n",
    "        sampled_examples = random.sample(exemplars, few_shot_size)\n",
    "\n",
    "        # Infer criteria based on sampled examples\n",
    "        hs1, cs1, r1, p1 = sampled_examples[0][0], sampled_examples[0][1], sampled_examples[0][2], sampled_examples[0][3]\n",
    "        hs2, cs2, r2, p2 = sampled_examples[1][0], sampled_examples[1][1], sampled_examples[1][2], sampled_examples[1][3]\n",
    "        hs3, cs3, r3, p3 = sampled_examples[2][0], sampled_examples[2][1], sampled_examples[2][2], sampled_examples[2][3]\n",
    "\n",
    "        few_shot_examples = cot_error_few_shot_tempate(aspect, hs1, cs1, r1, p1, hs2, cs2, r2, p2, hs3, cs3, r3, p3)\n",
    "        cot_refinement_prompt = candidate_cot_refinement_template(aspect, candidate_cot[0], few_shot_examples)\n",
    "        # print(cot_refinement_prompt)\n",
    "\n",
    "        refinement_prompts.append(cot_refinement_prompt)\n",
    "\n",
    "        candidate_cot_refined = predict(prompt=cot_refinement_prompt, system_description=\"\")\n",
    "        candidate_cot_refined = postprocess_llm_response(candidate_cot_refined)\n",
    "        candidate_cot_set.append(candidate_cot_refined)\n",
    "        \n",
    "    evaluated_candidates = [(candidate_cot, evaluate_candidate_cot(aspect, candidate_cot)[0], evaluate_candidate_cot(aspect, candidate_cot)[1]) for candidate_cot in candidate_cot_set]\n",
    "    evaluated_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_candidate = evaluated_candidates[0]\n",
    "    return top_candidate\n",
    "\n",
    "\n",
    "top_candidates_refined = []\n",
    "for candidate_cot in top_candidates:\n",
    "    print(candidate_cot)\n",
    "    refined_cot_candidate = iterative_refinement_of_cot_candidate(candidate_cot, aspect)\n",
    "    top_candidates_refined.append(refined_cot_candidate)\n",
    "\n",
    "top_candidates_refined.sort(key=lambda x: x[1], reverse=True)\n",
    "top_candidates = top_candidates_refined[:topk]\n",
    "top_candidate_global = top_candidates_refined[0]\n",
    "top_candidate_global"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
