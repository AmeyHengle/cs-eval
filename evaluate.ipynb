{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "os.environ['http_proxy'] = \"http://xen03.iitd.ac.in:3128\"\n",
    "os.environ['https_proxy'] = \"http://xen03.iitd.ac.in:3128\"\n",
    "os.environ['HF_TOKEN'] = \"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\"\n",
    "os.environ['OPENAI_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "os.environ['http_proxy'] = \"http://xen03.iitd.ac.in:3128\"\n",
    "os.environ['https_proxy'] = \"http://xen03.iitd.ac.in:3128\"\n",
    "os.environ['HF_TOKEN'] = \"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\"\n",
    "os.environ['OPENAI_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### G-EVAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e4d44625ee423fb1549382abcc15a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Write me a joke to make me laugh\n",
      "\n",
      "Cause I’m still sick, been sick all week\n",
      "\n",
      "Not sure when this illness started, but I've been sick all week. I slept on the flight from Los Angeles to London, got maybe an hour or two of shuteye, then was up at 3am on Friday, and I’ve stayed up ever since (except for five hours of sleep last night, or rather it wasn’t even sleeping, just trying to make my\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "class Mistral7B(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        model = self.load_model()\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "        return self.tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Mistral 7B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "mistral_7b = Mistral7B(model=model, tokenizer=tokenizer)\n",
    "print(mistral_7b.generate(\"Write me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "criteria = \"\"\"\n",
    "Your task is to rate the counterspeech written for a hateful statement (hate speech) on the metric - Relevance. \n",
    "Relevance (1-5) - This dimension evaluates whether the counterspeech addresses the central theme or subject of the hate speech.\n",
    "\"\"\"\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Relevance\",\n",
    "    criteria=criteria,\n",
    "    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n",
    "    # evaluation_steps=[\n",
    "    #     \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
    "    #     \"You should also heavily penalize omission of detail\",\n",
    "    #     \"Vague language, or contradicting OPINIONS, are OK\"\n",
    "    # ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT],\n",
    "    model='gpt-4'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<LLMTestCaseParams.INPUT: 'input'>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_metric.evaluation_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e899a34df52f45d1b4a28e1b1a8dca54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9988744405745063\n",
      "The counterspeech directly addresses the central theme of the hate speech and rates high in relevance.\n"
     ]
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "input = \"\"\"\n",
    "hatespeech: All muslims are terrorists.\n",
    "counterspeech: Not all Muslims are terrorists. They are peaceful people like us.\n",
    "\"\"\".strip()\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=input,\n",
    "    actual_output=\"1\",\n",
    "    # expected_output=\"1\",\n",
    "    # include_reason=False\n",
    ")\n",
    "\n",
    "correctness_metric.measure(test_case)\n",
    "print(correctness_metric.score)\n",
    "print(correctness_metric.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_metric.include_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2816317595814658"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_metric.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
