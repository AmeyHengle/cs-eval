,Model,Spearman Correlation (œÅ),t-statistic,p-value
4,GEVAL_Llama-3-8b-chat-hf_aggressiveness_score,-0.030402540952464467,-0.9608950539948368,0.3368376799483239
5,GEVAL_Mistral-7B-Instruct-v03_aggressiveness_score,0.35961612132159376,12.175199196534578,0.0
3,GEVAL_gpt-4_aggressiveness_score,0.061942404947514115,1.9605959449995005,0.050204020832644636
6,Llama-3-8b-chat-hf_aggressiveness_score,0.014054091211341943,0.44402858876153556,0.6571181724379711
7,Mistral-7B-Instruct-v03_aggressiveness_score,0.3804643946705361,12.996713650657624,0.0
9,aggressiveness_UniEval,-0.2671156700093846,-8.75666605813407,0.0
20,aq_score_(pred_cs),-0.07232093610852382,-2.2906990684563566,0.022188595389424304
24,"bart_score_(pred_cs, cs)",-0.1634500381226287,-5.23396104394928,2.0223740770930476e-07
18,"bert_score_(pred_cs, cs)",0.04913570858885108,1.5541301664945255,0.12047041198418107
10,"bleu_1_(pred_cs, cs)",-0.041360055991628626,-1.3077302523430312,0.19126597506922405
11,"bleu_2_(pred_cs, cs)",0.3133787833983408,10.425124572768492,0.0
12,"bleu_3_(pred_cs, cs)",0.31641026176824744,10.537132321699508,0.0
13,"bleu_4_(pred_cs, cs)",0.31641026176824744,10.537132321699508,0.0
0,gpt-4-zs_aggressiveness_score,0.4517055483752772,15.994641186159512,0.0
8,gpt-4_aggressiveness_score,0.2478580405576628,8.082315197824826,1.7763568394002505e-15
17,"meteor_score_(pred_cs, cs)",-0.2048658572979402,-6.612189719689879,6.159761589685786e-11
22,"negative_pc_score_(hs, pred_cs)",-0.22014530334679364,-7.129549241119455,1.930233750613297e-12
19,"pc_score_(hs, pred_cs)",0.22014530334679364,7.129549241119455,1.930233750613297e-12
21,"pd_score(hs, pred_cs)",-0.17086376655961244,-5.4783416879950355,5.435136896103643e-08
14,"rouge_1_(pred_cs, cs)",-0.10371901460216598,-3.294369427440259,0.0010211152788832667
15,"rouge_2_(pred_cs, cs)",-0.15833786539976125,-5.065980749057265,4.838723863809236e-07
16,"rouge_l_(pred_cs, cs)",-0.13730183035626456,-4.378993394703152,1.3180599664819326e-05
23,toxicity_(pred_cs),0.27938644262862516,9.192179832892936,0.0
1,zs_Llama-3-8b-chat-hf_aggressiveness_score,0.15695415824794726,5.020586174316575,6.098718519087498e-07
2,zs_Mistral-7B-Instruct-v03_aggressiveness_score,0.3284491546925488,10.985542300066863,0.0
